{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>HC in Cross-Domain Authorship Attribution Challenge</H1>\n",
    "Use this notebook to replicate the results reporter in the paper  </br>\n",
    "<ul>\n",
    "    [1] <a href = https://arxiv.org/abs/1911.01208>\n",
    "    Kipnis, A., ``Higher Criticism for Discriminating Word-Frequency Tables and Testing Authorship'', 2019\n",
    "    </a>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use HC-based test to attribute authorship in the PAN2018 cross-domain authorship attribution challenge https://pan.webis.de/clef18/pan18-web/author-identification.html#cross-domain\n",
    "- Only the English part (problems 1-4) of this challenge is considered. \n",
    "- We use a lemmatized version of the data obtained using the Stanford CoreNLP (https://stanfordnlp.github.io/CoreNLP/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import codecs\n",
    "from tqdm import tqdm\n",
    "\n",
    "#import auxiliary functions for python\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from AuthAttLib import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>Load Data</H2>\n",
    "<a href = https://www.uni-weimar.de/medien/webis/corpora/corpus-pan-labs-09-today/pan-18/pan18-data/pan18-cross-domain-authorship-attribution-test-dataset2-2018-04-20-password-protected.zip>Link</a> to dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset \n",
    "raw_data = pd.read_csv(\"../Data/PAN2018_probs_1_to_4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: remove proper names, cardinal digits, and punctuation\n",
    "from text_processing import remove_parts_of_speach\n",
    "punct = [':',';','\"','(',')','-',',','.','`','\\`','!']\n",
    "\n",
    "def text_proc_loc(text) : \n",
    "    return remove_parts_of_speach(text, \n",
    "                        to_remove=['NNP', 'NNPS', 'CD']\n",
    "                    )\n",
    "def lemmatize_vocab(list_of_words) :\n",
    "    return text_proc_loc(\" \".join(list_of_words)).split()\n",
    "\n",
    "def get_n_most_common_words(n = 5000) :\n",
    "    most_common_list = pd.read_csv('~/Data/5000_most_common_english_words.csv')\n",
    "    return list(set(most_common_list.Word.tolist()))[:n]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore proper names, numbers, and some pronouns: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = raw_data.filter(['dataset', 'author', 'doc_id', 'prob'])\n",
    "data.loc[:,'split'] = 'train'\n",
    "data.loc[data.doc_id.str.find('test')>-1,'split'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove POS: proper names and cardinal digits (can take a while)\n",
    "data.loc[:,'text'] = raw_data.text.apply(text_proc_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>Multi-author model</H2>\n",
    "\n",
    "- For each problem in the challenge, train a model and evaluate it over a test set. <br>\n",
    "- The following implementation opt not to use the UNNKOWN option, hence the recall is always 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Creating author-model for candidate00001 using 3000 features...\n",
      "\t\tfound 7 documents and 5316 relevant tokens.\n",
      "\t Creating author-model for candidate00002 using 3000 features...\n",
      "\t\tfound 7 documents and 5012 relevant tokens.\n",
      "\t Creating author-model for candidate00003 using 3000 features...\n",
      "\t\tfound 7 documents and 5093 relevant tokens.\n",
      "\t Creating author-model for candidate00004 using 3000 features...\n",
      "\t\tfound 7 documents and 5097 relevant tokens.\n",
      "\t Creating author-model for candidate00005 using 3000 features...\n",
      "\t\tfound 7 documents and 4578 relevant tokens.\n",
      "\t Creating author-model for candidate00006 using 3000 features...\n",
      "\t\tfound 7 documents and 5095 relevant tokens.\n",
      "\t Creating author-model for candidate00007 using 3000 features...\n",
      "\t\tfound 7 documents and 5221 relevant tokens.\n",
      "\t Creating author-model for candidate00008 using 3000 features...\n",
      "\t\tfound 7 documents and 5526 relevant tokens.\n",
      "\t Creating author-model for candidate00009 using 3000 features...\n",
      "\t\tfound 7 documents and 5203 relevant tokens.\n",
      "\t Creating author-model for candidate00010 using 3000 features...\n",
      "\t\tfound 7 documents and 5336 relevant tokens.\n",
      "\t Creating author-model for candidate00011 using 3000 features...\n",
      "\t\tfound 7 documents and 4385 relevant tokens.\n",
      "\t Creating author-model for candidate00012 using 3000 features...\n",
      "\t\tfound 7 documents and 5170 relevant tokens.\n",
      "\t Creating author-model for candidate00013 using 3000 features...\n",
      "\t\tfound 7 documents and 4871 relevant tokens.\n",
      "\t Creating author-model for candidate00014 using 3000 features...\n",
      "\t\tfound 7 documents and 4937 relevant tokens.\n",
      "\t Creating author-model for candidate00015 using 3000 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/79 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tfound 7 documents and 5194 relevant tokens.\n",
      "\t Creating author-model for candidate00016 using 3000 features...\n",
      "\t\tfound 7 documents and 4719 relevant tokens.\n",
      "\t Creating author-model for candidate00017 using 3000 features...\n",
      "\t\tfound 7 documents and 5218 relevant tokens.\n",
      "\t Creating author-model for candidate00018 using 3000 features...\n",
      "\t\tfound 7 documents and 5033 relevant tokens.\n",
      "\t Creating author-model for candidate00019 using 3000 features...\n",
      "\t\tfound 7 documents and 4742 relevant tokens.\n",
      "\t Creating author-model for candidate00020 using 3000 features...\n",
      "\t\tfound 7 documents and 4905 relevant tokens.\n",
      "Evaluate on test set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  3%|▎         | 2/79 [00:00<00:04, 19.07it/s]\u001b[A\n",
      "  5%|▌         | 4/79 [00:00<00:03, 19.05it/s]\u001b[A\n",
      "  9%|▉         | 7/79 [00:00<00:03, 20.08it/s]\u001b[A\n",
      " 13%|█▎        | 10/79 [00:00<00:03, 20.97it/s]\u001b[A\n",
      " 16%|█▋        | 13/79 [00:00<00:03, 21.58it/s]\u001b[A\n",
      " 20%|██        | 16/79 [00:00<00:02, 22.04it/s]\u001b[A\n",
      " 24%|██▍       | 19/79 [00:00<00:02, 22.45it/s]\u001b[A\n",
      " 28%|██▊       | 22/79 [00:01<00:02, 21.87it/s]\u001b[A\n",
      " 32%|███▏      | 25/79 [00:01<00:02, 20.50it/s]\u001b[A\n",
      " 34%|███▍      | 27/79 [00:01<00:02, 20.33it/s]\u001b[A\n",
      " 38%|███▊      | 30/79 [00:01<00:02, 21.04it/s]\u001b[A\n",
      " 42%|████▏     | 33/79 [00:01<00:02, 21.69it/s]\u001b[A\n",
      " 46%|████▌     | 36/79 [00:01<00:01, 21.99it/s]\u001b[A\n",
      " 49%|████▉     | 39/79 [00:01<00:01, 22.25it/s]\u001b[A\n",
      " 53%|█████▎    | 42/79 [00:01<00:01, 22.71it/s]\u001b[A\n",
      " 57%|█████▋    | 45/79 [00:02<00:01, 23.13it/s]\u001b[A\n",
      " 61%|██████    | 48/79 [00:02<00:01, 22.95it/s]\u001b[A\n",
      " 65%|██████▍   | 51/79 [00:02<00:01, 22.95it/s]\u001b[A\n",
      " 68%|██████▊   | 54/79 [00:02<00:01, 23.13it/s]\u001b[A\n",
      " 72%|███████▏  | 57/79 [00:02<00:00, 22.99it/s]\u001b[A\n",
      " 76%|███████▌  | 60/79 [00:02<00:00, 22.93it/s]\u001b[A\n",
      " 80%|███████▉  | 63/79 [00:02<00:00, 22.85it/s]\u001b[A\n",
      " 84%|████████▎ | 66/79 [00:02<00:00, 21.41it/s]\u001b[A\n",
      " 87%|████████▋ | 69/79 [00:03<00:00, 20.22it/s]\u001b[A\n",
      " 91%|█████████ | 72/79 [00:03<00:00, 20.96it/s]\u001b[A\n",
      " 95%|█████████▍| 75/79 [00:03<00:00, 21.97it/s]\u001b[A\n",
      " 99%|█████████▊| 78/79 [00:03<00:00, 22.36it/s]\u001b[A\n",
      " 25%|██▌       | 1/4 [00:04<00:12,  4.12s/it]s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem = problem00001\n",
      "recall = 1.0\n",
      "accuracy = 0.5443037974683544\n",
      "F1 = 0.7049180327868853\n",
      "\t Creating author-model for candidate00001 using 3000 features...\n",
      "\t\tfound 7 documents and 5364 relevant tokens.\n",
      "\t Creating author-model for candidate00002 using 3000 features...\n",
      "\t\tfound 7 documents and 5257 relevant tokens.\n",
      "\t Creating author-model for candidate00003 using 3000 features...\n",
      "\t\tfound 7 documents and 5110 relevant tokens.\n",
      "\t Creating author-model for candidate00004 using 3000 features...\n",
      "\t\tfound 7 documents and 5014 relevant tokens.\n",
      "\t Creating author-model for candidate00005 using 3000 features...\n",
      "\t\tfound 7 documents and 4746 relevant tokens.\n",
      "\t Creating author-model for candidate00006 using 3000 features...\n",
      "\t\tfound 7 documents and 4957 relevant tokens.\n",
      "\t Creating author-model for candidate00007 using 3000 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/74 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tfound 7 documents and 5111 relevant tokens.\n",
      "\t Creating author-model for candidate00008 using 3000 features...\n",
      "\t\tfound 7 documents and 5034 relevant tokens.\n",
      "\t Creating author-model for candidate00009 using 3000 features...\n",
      "\t\tfound 7 documents and 4416 relevant tokens.\n",
      "\t Creating author-model for candidate00010 using 3000 features...\n",
      "\t\tfound 7 documents and 4880 relevant tokens.\n",
      "\t Creating author-model for candidate00011 using 3000 features...\n",
      "\t\tfound 7 documents and 4619 relevant tokens.\n",
      "\t Creating author-model for candidate00012 using 3000 features...\n",
      "\t\tfound 7 documents and 5173 relevant tokens.\n",
      "\t Creating author-model for candidate00013 using 3000 features...\n",
      "\t\tfound 7 documents and 4921 relevant tokens.\n",
      "\t Creating author-model for candidate00014 using 3000 features...\n",
      "\t\tfound 7 documents and 5230 relevant tokens.\n",
      "\t Creating author-model for candidate00015 using 3000 features...\n",
      "\t\tfound 7 documents and 5327 relevant tokens.\n",
      "Evaluate on test set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  4%|▍         | 3/74 [00:00<00:02, 27.14it/s]\u001b[A\n",
      "  8%|▊         | 6/74 [00:00<00:02, 26.78it/s]\u001b[A\n",
      " 14%|█▎        | 10/74 [00:00<00:02, 27.86it/s]\u001b[A\n",
      " 18%|█▊        | 13/74 [00:00<00:02, 28.11it/s]\u001b[A\n",
      " 23%|██▎       | 17/74 [00:00<00:01, 29.41it/s]\u001b[A\n",
      " 27%|██▋       | 20/74 [00:00<00:01, 29.54it/s]\u001b[A\n",
      " 31%|███       | 23/74 [00:00<00:01, 29.04it/s]\u001b[A\n",
      " 36%|███▋      | 27/74 [00:00<00:01, 28.87it/s]\u001b[A\n",
      " 41%|████      | 30/74 [00:01<00:01, 26.33it/s]\u001b[A\n",
      " 45%|████▍     | 33/74 [00:01<00:01, 25.69it/s]\u001b[A\n",
      " 49%|████▊     | 36/74 [00:01<00:01, 26.40it/s]\u001b[A\n",
      " 53%|█████▎    | 39/74 [00:01<00:01, 25.42it/s]\u001b[A\n",
      " 57%|█████▋    | 42/74 [00:01<00:01, 23.98it/s]\u001b[A\n",
      " 61%|██████    | 45/74 [00:01<00:01, 24.57it/s]\u001b[A\n",
      " 65%|██████▍   | 48/74 [00:01<00:01, 25.98it/s]\u001b[A\n",
      " 70%|███████   | 52/74 [00:01<00:00, 27.19it/s]\u001b[A\n",
      " 74%|███████▍  | 55/74 [00:02<00:00, 25.44it/s]\u001b[A\n",
      " 78%|███████▊  | 58/74 [00:02<00:00, 24.89it/s]\u001b[A\n",
      " 82%|████████▏ | 61/74 [00:02<00:00, 26.14it/s]\u001b[A\n",
      " 88%|████████▊ | 65/74 [00:02<00:00, 27.57it/s]\u001b[A\n",
      " 92%|█████████▏| 68/74 [00:02<00:00, 28.18it/s]\u001b[A\n",
      " 96%|█████████▌| 71/74 [00:02<00:00, 28.59it/s]\u001b[A\n",
      " 50%|█████     | 2/4 [00:07<00:07,  3.81s/it]s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem = problem00002\n",
      "recall = 1.0\n",
      "accuracy = 0.5675675675675675\n",
      "F1 = 0.7241379310344828\n",
      "\t Creating author-model for candidate00001 using 3000 features...\n",
      "\t\tfound 7 documents and 4497 relevant tokens.\n",
      "\t Creating author-model for candidate00002 using 3000 features...\n",
      "\t\tfound 7 documents and 5392 relevant tokens.\n",
      "\t Creating author-model for candidate00003 using 3000 features...\n",
      "\t\tfound 7 documents and 5316 relevant tokens.\n",
      "\t Creating author-model for candidate00004 using 3000 features...\n",
      "\t\tfound 7 documents and 4799 relevant tokens.\n",
      "\t Creating author-model for candidate00005 using 3000 features...\n",
      "\t\tfound 7 documents and 5191 relevant tokens.\n",
      "\t Creating author-model for candidate00006 using 3000 features...\n",
      "\t\tfound 7 documents and 4977 relevant tokens.\n",
      "\t Creating author-model for candidate00007 using 3000 features...\n",
      "\t\tfound 7 documents and 4639 relevant tokens.\n",
      "\t Creating author-model for candidate00008 using 3000 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█▎        | 5/40 [00:00<00:00, 41.84it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tfound 7 documents and 5048 relevant tokens.\n",
      "\t Creating author-model for candidate00009 using 3000 features...\n",
      "\t\tfound 7 documents and 4926 relevant tokens.\n",
      "\t Creating author-model for candidate00010 using 3000 features...\n",
      "\t\tfound 7 documents and 5417 relevant tokens.\n",
      "Evaluate on test set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 22%|██▎       | 9/40 [00:00<00:00, 40.47it/s]\u001b[A\n",
      " 32%|███▎      | 13/40 [00:00<00:00, 38.92it/s]\u001b[A\n",
      " 42%|████▎     | 17/40 [00:00<00:00, 36.10it/s]\u001b[A\n",
      " 52%|█████▎    | 21/40 [00:00<00:00, 34.65it/s]\u001b[A\n",
      " 62%|██████▎   | 25/40 [00:00<00:00, 34.40it/s]\u001b[A\n",
      " 70%|███████   | 28/40 [00:00<00:00, 31.58it/s]\u001b[A\n",
      " 78%|███████▊  | 31/40 [00:00<00:00, 30.47it/s]\u001b[A\n",
      " 85%|████████▌ | 34/40 [00:01<00:00, 30.08it/s]\u001b[A\n",
      " 92%|█████████▎| 37/40 [00:01<00:00, 29.62it/s]\u001b[A\n",
      " 75%|███████▌  | 3/4 [00:08<00:03,  3.11s/it]s]\u001b[A\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem = problem00003\n",
      "recall = 1.0\n",
      "accuracy = 0.75\n",
      "F1 = 0.8571428571428571\n",
      "\t Creating author-model for candidate00001 using 3000 features...\n",
      "\t\tfound 7 documents and 5654 relevant tokens.\n",
      "\t Creating author-model for candidate00002 using 3000 features...\n",
      "\t\tfound 7 documents and 5230 relevant tokens.\n",
      "\t Creating author-model for candidate00003 using 3000 features...\n",
      "\t\tfound 7 documents and 5494 relevant tokens.\n",
      "\t Creating author-model for candidate00004 using 3000 features...\n",
      "\t\tfound 7 documents and 5503 relevant tokens.\n",
      "\t Creating author-model for candidate00005 using 3000 features...\n",
      "\t\tfound 7 documents and 4838 relevant tokens.\n",
      "Evaluate on test set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 38%|███▊      | 6/16 [00:00<00:00, 52.51it/s]\u001b[A\n",
      " 81%|████████▏ | 13/16 [00:00<00:00, 56.24it/s]\u001b[A\n",
      "100%|██████████| 4/4 [00:09<00:00,  2.29s/it]s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem = problem00004\n",
      "recall = 1.0\n",
      "accuracy = 0.875\n",
      "F1 = 0.9333333333333333\n",
      "\n",
      " mean F1 =  0.8048830385743897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lo_problem = pd.unique(data['prob'])\n",
    "\n",
    "lo_F1 = []\n",
    "lo_acc = []\n",
    "\n",
    "for prob in tqdm(lo_problem[:4]) :\n",
    "    data_prob = data[data['prob'] == prob]\n",
    "    \n",
    "    data_train = data_prob[data_prob['split'] == 'train']\n",
    "    #compute model for each problem:\n",
    "    model = AuthorshipAttributionMulti(\n",
    "        data_train,        #dataset arrange in a data frame\n",
    "        vocab_size=3000,  #uses 3000 most frequent ngram\n",
    "        stbl=True,       #type of HC statistic\n",
    "        ngram_range=(1,1), #mono-, bi-, and tri- grams\n",
    "        randomize=False     #use randomized p-values\n",
    "        )\n",
    "    \n",
    "    #attribute test documents:\n",
    "    print(\"Evaluate on test set:\")\n",
    "    data_test = data_prob[data_prob['split'] == 'test']\n",
    "    lo_test_docs = pd.unique(data_test.doc_id)\n",
    "    df = pd.DataFrame() #save results in this dataframe\n",
    "\n",
    "    for doc in tqdm(lo_test_docs) :\n",
    "        sm = data_test[data_test.doc_id == doc]\n",
    "        \n",
    "        pred,_ = model.predict(sm.text.values[0], unk_thresh = 1e6, method = 'HC') \n",
    "                # can use 'unk_thresh' to get '<UNK>' instead of the name \n",
    "                # of the corpus with smallest HC in the case when the smallest\n",
    "                # HC is above 'unk_thresh'. \n",
    "\n",
    "        auth = sm.author.values[0]\n",
    "        df = df.append({'doc_id' : doc,\n",
    "                   'author' : auth,\n",
    "                   'predicted' : pred,\n",
    "                  }, ignore_index = True)\n",
    "\n",
    "\n",
    "    # evaluate accuracy and F1 score\n",
    "    df_r = df[df.predicted != '<UNK>']\n",
    "    recall = len(df_r) / len(df)\n",
    "    acc = np.mean((df_r.predicted == df_r.author).values)\n",
    "    \n",
    "    print(\"problem = {}\".format(prob))\n",
    "    print(\"recall = {}\".format(recall))\n",
    "    print(\"accuracy = {}\".format(acc))\n",
    "    print(\"F1 = {}\".format(2*recall*acc / (recall + acc)))\n",
    "    lo_F1 += [2*recall*acc / (recall + acc)]\n",
    "    lo_acc += [acc]\n",
    "\n",
    "#prob1: |W| = 1500, ng = (1,3) --> F1 = 0.661, acc = 0.493\n",
    "#prob2: |W| = 1500, ng = (1,3) --> F1 = 0.678, acc = 0.513\n",
    "#prob3: |W| = 1500, ng = (1,3) --> F1 = 0.7878, acc = 0.65\n",
    "#prob4: |W| = 1500, ng = (1,3) --> F1 = 0.814, acc = 0.6875\n",
    "\n",
    "print(\"\\n mean F1 = \", np.mean(lo_F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ggplot: (293430873)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/plotnine/ggplot.py:706: UserWarning: Saving 6.4 x 4.8 in image.\n",
      "  from_inches(height, units), units))\n",
      "/usr/local/lib/python3.7/site-packages/plotnine/ggplot.py:707: UserWarning: Filename: /Users/kipnisal/Dropbox/Apps//Overleaf/Higher Criticism and Authorship Attribution (presentation)/Figs/PAN2018_F1.png\n",
      "  warn('Filename: {}'.format(filename))\n"
     ]
    }
   ],
   "source": [
    "PAN2018_English_results = {\n",
    "'Custódio and Paraboni' : 0.744,\n",
    "'Murauer et al.' : 0.762,\n",
    "'Halvani and Graner' :  0.679,\n",
    "'Mosavat' : 0.685,\n",
    "'Yigal et al.' : 0.672,\n",
    "'Martín dCR et al.' : 0.601,\n",
    "'PAN18-BASELINE' : 0.697,\n",
    "'Miller et al.' : 0.573,\n",
    "'Schaetti' :  0.538}\n",
    "\n",
    "#from \n",
    "\n",
    "from plotnine import *\n",
    "\n",
    "PAN2018_English_results['HC-based test'] = np.mean(lo_F1)\n",
    "\n",
    "df = pd.DataFrame.from_dict(PAN2018_English_results, orient='index')\n",
    "df = df.rename(columns = {0 : 'F1'}).reset_index()\n",
    "dfs = df.sort_values('F1').reset_index()\n",
    "df['cat'] = pd.Categorical(df.F1, categories=dfs['F1'].values, ordered=True)\n",
    "\n",
    "\n",
    "df['type'] = 'other'\n",
    "df.loc[df['index'] == 'HC-based test','type']='HC'\n",
    "\n",
    "p = (ggplot(aes(x = 'cat', y = 'F1', fill = 'type', label = 'index'), data = df)\n",
    "     + geom_bar(position='dodge', stat=\"identity\", show_legend=False)\n",
    "     + geom_text(nudge_y = -0.25) + coord_flip() + ggtitle('PAN 2018 Authorship Challenge')\n",
    "     + xlab('') + theme(axis_text_y=element_blank())\n",
    "     + ylab('F1 score')\n",
    "    )\n",
    "print(p)\n",
    "\n",
    "path_to_plots = \"/Users/kipnisal/Dropbox/Apps//Overleaf/Higher Criticism and Authorship Attribution (presentation)/Figs/\"\n",
    "path_to_plots =\"\"\n",
    "p.save(path_to_plots + 'PAN2018_F1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Multi-author with head-to-head comparisons</h2>\n",
    "\n",
    "Compare each pair of corpora. Use only distinguishing features of two corpora in testing. Attribute tested docuement to whichever corpus has most number of wins in all pairwise comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 45 author-pairs\n",
      "MultiBinaryAuthorModel: Creating model for candidate00001 vs candidate00002...\n",
      "\t Creating author-model for candidate00001 using 1500 features...\n",
      "\t\tfound 7 documents and 8126 relevant tokens.\n",
      "\t Creating author-model for candidate00002 using 1500 features...\n",
      "\t\tfound 7 documents and 9304 relevant tokens.\n",
      "Changing vocabulary for candidate00001. Found 1335 relevant tokens.\n",
      "Changing vocabulary for candidate00002. Found 1496 relevant tokens.\n",
      "Reduced to 51 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00001 vs candidate00003...\n",
      "\t Creating author-model for candidate00001 using 1500 features...\n",
      "\t\tfound 7 documents and 8145 relevant tokens.\n",
      "\t Creating author-model for candidate00003 using 1500 features...\n",
      "\t\tfound 7 documents and 7406 relevant tokens.\n",
      "Changing vocabulary for candidate00001. Found 1393 relevant tokens.\n",
      "Changing vocabulary for candidate00003. Found 1446 relevant tokens.\n",
      "Reduced to 41 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00001 vs candidate00004...\n",
      "\t Creating author-model for candidate00001 using 1500 features...\n",
      "\t\tfound 7 documents and 8133 relevant tokens.\n",
      "\t Creating author-model for candidate00004 using 1500 features...\n",
      "\t\tfound 7 documents and 9282 relevant tokens.\n",
      "Changing vocabulary for candidate00001. Found 1741 relevant tokens.\n",
      "Changing vocabulary for candidate00004. Found 1745 relevant tokens.\n",
      "Reduced to 59 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00001 vs candidate00005...\n",
      "\t Creating author-model for candidate00001 using 1500 features...\n",
      "\t\tfound 7 documents and 7871 relevant tokens.\n",
      "\t Creating author-model for candidate00005 using 1500 features...\n",
      "\t\tfound 7 documents and 9518 relevant tokens.\n",
      "Changing vocabulary for candidate00001. Found 1192 relevant tokens.\n",
      "Changing vocabulary for candidate00005. Found 1629 relevant tokens.\n",
      "Reduced to 65 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00001 vs candidate00006...\n",
      "\t Creating author-model for candidate00001 using 1500 features...\n",
      "\t\tfound 7 documents and 8131 relevant tokens.\n",
      "\t Creating author-model for candidate00006 using 1500 features...\n",
      "\t\tfound 7 documents and 8788 relevant tokens.\n",
      "Changing vocabulary for candidate00001. Found 1574 relevant tokens.\n",
      "Changing vocabulary for candidate00006. Found 2153 relevant tokens.\n",
      "Reduced to 64 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00001 vs candidate00007...\n",
      "\t Creating author-model for candidate00001 using 1500 features...\n",
      "\t\tfound 7 documents and 8123 relevant tokens.\n",
      "\t Creating author-model for candidate00007 using 1500 features...\n",
      "\t\tfound 7 documents and 8431 relevant tokens.\n",
      "Changing vocabulary for candidate00001. Found 735 relevant tokens.\n",
      "Changing vocabulary for candidate00007. Found 892 relevant tokens.\n",
      "Reduced to 49 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00001 vs candidate00008...\n",
      "\t Creating author-model for candidate00001 using 1500 features...\n",
      "\t\tfound 7 documents and 7899 relevant tokens.\n",
      "\t Creating author-model for candidate00008 using 1500 features...\n",
      "\t\tfound 7 documents and 9509 relevant tokens.\n",
      "Changing vocabulary for candidate00001. Found 2529 relevant tokens.\n",
      "Changing vocabulary for candidate00008. Found 3133 relevant tokens.\n",
      "Reduced to 80 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00001 vs candidate00009...\n",
      "\t Creating author-model for candidate00001 using 1500 features...\n",
      "\t\tfound 7 documents and 8144 relevant tokens.\n",
      "\t Creating author-model for candidate00009 using 1500 features...\n",
      "\t\tfound 7 documents and 8259 relevant tokens.\n",
      "Changing vocabulary for candidate00001. Found 1148 relevant tokens.\n",
      "Changing vocabulary for candidate00009. Found 1067 relevant tokens.\n",
      "Reduced to 53 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00001 vs candidate00010...\n",
      "\t Creating author-model for candidate00001 using 1500 features...\n",
      "\t\tfound 7 documents and 8015 relevant tokens.\n",
      "\t Creating author-model for candidate00010 using 1500 features...\n",
      "\t\tfound 7 documents and 9398 relevant tokens.\n",
      "Changing vocabulary for candidate00001. Found 1431 relevant tokens.\n",
      "Changing vocabulary for candidate00010. Found 1954 relevant tokens.\n",
      "Reduced to 63 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00002 vs candidate00003...\n",
      "\t Creating author-model for candidate00002 using 1500 features...\n",
      "\t\tfound 7 documents and 9387 relevant tokens.\n",
      "\t Creating author-model for candidate00003 using 1500 features...\n",
      "\t\tfound 7 documents and 7502 relevant tokens.\n",
      "Changing vocabulary for candidate00002. Found 4899 relevant tokens.\n",
      "Changing vocabulary for candidate00003. Found 4094 relevant tokens.\n",
      "Reduced to 413 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00002 vs candidate00004...\n",
      "\t Creating author-model for candidate00002 using 1500 features...\n",
      "\t\tfound 7 documents and 9427 relevant tokens.\n",
      "\t Creating author-model for candidate00004 using 1500 features...\n",
      "\t\tfound 7 documents and 9383 relevant tokens.\n",
      "Changing vocabulary for candidate00002. Found 3121 relevant tokens.\n",
      "Changing vocabulary for candidate00004. Found 3149 relevant tokens.\n",
      "Reduced to 307 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00002 vs candidate00005...\n",
      "\t Creating author-model for candidate00002 using 1500 features...\n",
      "\t\tfound 7 documents and 9185 relevant tokens.\n",
      "\t Creating author-model for candidate00005 using 1500 features...\n",
      "\t\tfound 7 documents and 9666 relevant tokens.\n",
      "Changing vocabulary for candidate00002. Found 3197 relevant tokens.\n",
      "Changing vocabulary for candidate00005. Found 2991 relevant tokens.\n",
      "Reduced to 263 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00002 vs candidate00006...\n",
      "\t Creating author-model for candidate00002 using 1500 features...\n",
      "\t\tfound 7 documents and 9396 relevant tokens.\n",
      "\t Creating author-model for candidate00006 using 1500 features...\n",
      "\t\tfound 7 documents and 8906 relevant tokens.\n",
      "Changing vocabulary for candidate00002. Found 1606 relevant tokens.\n",
      "Changing vocabulary for candidate00006. Found 1850 relevant tokens.\n",
      "Reduced to 165 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00002 vs candidate00007...\n",
      "\t Creating author-model for candidate00002 using 1500 features...\n",
      "\t\tfound 7 documents and 9393 relevant tokens.\n",
      "\t Creating author-model for candidate00007 using 1500 features...\n",
      "\t\tfound 7 documents and 8528 relevant tokens.\n",
      "Changing vocabulary for candidate00002. Found 1822 relevant tokens.\n",
      "Changing vocabulary for candidate00007. Found 1673 relevant tokens.\n",
      "Reduced to 33 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00002 vs candidate00008...\n",
      "\t Creating author-model for candidate00002 using 1500 features...\n",
      "\t\tfound 7 documents and 9208 relevant tokens.\n",
      "\t Creating author-model for candidate00008 using 1500 features...\n",
      "\t\tfound 7 documents and 9607 relevant tokens.\n",
      "Changing vocabulary for candidate00002. Found 2374 relevant tokens.\n",
      "Changing vocabulary for candidate00008. Found 2047 relevant tokens.\n",
      "Reduced to 56 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00002 vs candidate00009...\n",
      "\t Creating author-model for candidate00002 using 1500 features...\n",
      "\t\tfound 7 documents and 9398 relevant tokens.\n",
      "\t Creating author-model for candidate00009 using 1500 features...\n",
      "\t\tfound 7 documents and 8336 relevant tokens.\n",
      "Changing vocabulary for candidate00002. Found 1788 relevant tokens.\n",
      "Changing vocabulary for candidate00009. Found 1293 relevant tokens.\n",
      "Reduced to 32 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00002 vs candidate00010...\n",
      "\t Creating author-model for candidate00002 using 1500 features...\n",
      "\t\tfound 7 documents and 9315 relevant tokens.\n",
      "\t Creating author-model for candidate00010 using 1500 features...\n",
      "\t\tfound 7 documents and 9510 relevant tokens.\n",
      "Changing vocabulary for candidate00002. Found 1080 relevant tokens.\n",
      "Changing vocabulary for candidate00010. Found 1394 relevant tokens.\n",
      "Reduced to 38 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00003 vs candidate00004...\n",
      "\t Creating author-model for candidate00003 using 1500 features...\n",
      "\t\tfound 7 documents and 7403 relevant tokens.\n",
      "\t Creating author-model for candidate00004 using 1500 features...\n",
      "\t\tfound 7 documents and 9331 relevant tokens.\n",
      "Changing vocabulary for candidate00003. Found 2540 relevant tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing vocabulary for candidate00004. Found 3163 relevant tokens.\n",
      "Reduced to 53 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00003 vs candidate00005...\n",
      "\t Creating author-model for candidate00003 using 1500 features...\n",
      "\t\tfound 7 documents and 7274 relevant tokens.\n",
      "\t Creating author-model for candidate00005 using 1500 features...\n",
      "\t\tfound 7 documents and 9705 relevant tokens.\n",
      "Changing vocabulary for candidate00003. Found 3711 relevant tokens.\n",
      "Changing vocabulary for candidate00005. Found 4560 relevant tokens.\n",
      "Reduced to 532 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00003 vs candidate00006...\n",
      "\t Creating author-model for candidate00003 using 1500 features...\n",
      "\t\tfound 7 documents and 7517 relevant tokens.\n",
      "\t Creating author-model for candidate00006 using 1500 features...\n",
      "\t\tfound 7 documents and 8901 relevant tokens.\n",
      "Changing vocabulary for candidate00003. Found 3627 relevant tokens.\n",
      "Changing vocabulary for candidate00006. Found 4180 relevant tokens.\n",
      "Reduced to 265 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00003 vs candidate00007...\n",
      "\t Creating author-model for candidate00003 using 1500 features...\n",
      "\t\tfound 7 documents and 7429 relevant tokens.\n",
      "\t Creating author-model for candidate00007 using 1500 features...\n",
      "\t\tfound 7 documents and 8511 relevant tokens.\n",
      "Changing vocabulary for candidate00003. Found 2099 relevant tokens.\n",
      "Changing vocabulary for candidate00007. Found 2292 relevant tokens.\n",
      "Reduced to 75 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00003 vs candidate00008...\n",
      "\t Creating author-model for candidate00003 using 1500 features...\n",
      "\t\tfound 7 documents and 7265 relevant tokens.\n",
      "\t Creating author-model for candidate00008 using 1500 features...\n",
      "\t\tfound 7 documents and 9627 relevant tokens.\n",
      "Changing vocabulary for candidate00003. Found 2682 relevant tokens.\n",
      "Changing vocabulary for candidate00008. Found 3055 relevant tokens.\n",
      "Reduced to 66 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00003 vs candidate00009...\n",
      "\t Creating author-model for candidate00003 using 1500 features...\n",
      "\t\tfound 7 documents and 7483 relevant tokens.\n",
      "\t Creating author-model for candidate00009 using 1500 features...\n",
      "\t\tfound 7 documents and 8316 relevant tokens.\n",
      "Changing vocabulary for candidate00003. Found 3726 relevant tokens.\n",
      "Changing vocabulary for candidate00009. Found 3967 relevant tokens.\n",
      "Reduced to 506 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00003 vs candidate00010...\n",
      "\t Creating author-model for candidate00003 using 1500 features...\n",
      "\t\tfound 7 documents and 7397 relevant tokens.\n",
      "\t Creating author-model for candidate00010 using 1500 features...\n",
      "\t\tfound 7 documents and 9495 relevant tokens.\n",
      "Changing vocabulary for candidate00003. Found 2255 relevant tokens.\n",
      "Changing vocabulary for candidate00010. Found 3063 relevant tokens.\n",
      "Reduced to 43 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00004 vs candidate00005...\n",
      "\t Creating author-model for candidate00004 using 1500 features...\n",
      "\t\tfound 7 documents and 9134 relevant tokens.\n",
      "\t Creating author-model for candidate00005 using 1500 features...\n",
      "\t\tfound 7 documents and 9567 relevant tokens.\n",
      "Changing vocabulary for candidate00004. Found 2631 relevant tokens.\n",
      "Changing vocabulary for candidate00005. Found 2692 relevant tokens.\n",
      "Reduced to 90 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00004 vs candidate00006...\n",
      "\t Creating author-model for candidate00004 using 1500 features...\n",
      "\t\tfound 7 documents and 9369 relevant tokens.\n",
      "\t Creating author-model for candidate00006 using 1500 features...\n",
      "\t\tfound 7 documents and 8891 relevant tokens.\n",
      "Changing vocabulary for candidate00004. Found 924 relevant tokens.\n",
      "Changing vocabulary for candidate00006. Found 1070 relevant tokens.\n",
      "Reduced to 42 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00004 vs candidate00007...\n",
      "\t Creating author-model for candidate00004 using 1500 features...\n",
      "\t\tfound 7 documents and 9341 relevant tokens.\n",
      "\t Creating author-model for candidate00007 using 1500 features...\n",
      "\t\tfound 7 documents and 8500 relevant tokens.\n",
      "Changing vocabulary for candidate00004. Found 1845 relevant tokens.\n",
      "Changing vocabulary for candidate00007. Found 1850 relevant tokens.\n",
      "Reduced to 44 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00004 vs candidate00008...\n",
      "\t Creating author-model for candidate00004 using 1500 features...\n",
      "\t\tfound 7 documents and 9135 relevant tokens.\n",
      "\t Creating author-model for candidate00008 using 1500 features...\n",
      "\t\tfound 7 documents and 9625 relevant tokens.\n",
      "Changing vocabulary for candidate00004. Found 1602 relevant tokens.\n",
      "Changing vocabulary for candidate00008. Found 1537 relevant tokens.\n",
      "Reduced to 59 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00004 vs candidate00009...\n",
      "\t Creating author-model for candidate00004 using 1500 features...\n",
      "\t\tfound 7 documents and 9376 relevant tokens.\n",
      "\t Creating author-model for candidate00009 using 1500 features...\n",
      "\t\tfound 7 documents and 8371 relevant tokens.\n",
      "Changing vocabulary for candidate00004. Found 1512 relevant tokens.\n",
      "Changing vocabulary for candidate00009. Found 1258 relevant tokens.\n",
      "Reduced to 33 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00004 vs candidate00010...\n",
      "\t Creating author-model for candidate00004 using 1500 features...\n",
      "\t\tfound 7 documents and 9243 relevant tokens.\n",
      "\t Creating author-model for candidate00010 using 1500 features...\n",
      "\t\tfound 7 documents and 9418 relevant tokens.\n",
      "Changing vocabulary for candidate00004. Found 2137 relevant tokens.\n",
      "Changing vocabulary for candidate00010. Found 2392 relevant tokens.\n",
      "Reduced to 60 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00005 vs candidate00006...\n",
      "\t Creating author-model for candidate00005 using 1500 features...\n",
      "\t\tfound 7 documents and 9635 relevant tokens.\n",
      "\t Creating author-model for candidate00006 using 1500 features...\n",
      "\t\tfound 7 documents and 8618 relevant tokens.\n",
      "Changing vocabulary for candidate00005. Found 1978 relevant tokens.\n",
      "Changing vocabulary for candidate00006. Found 2207 relevant tokens.\n",
      "Reduced to 80 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00005 vs candidate00007...\n",
      "\t Creating author-model for candidate00005 using 1500 features...\n",
      "\t\tfound 7 documents and 9600 relevant tokens.\n",
      "\t Creating author-model for candidate00007 using 1500 features...\n",
      "\t\tfound 7 documents and 8265 relevant tokens.\n",
      "Changing vocabulary for candidate00005. Found 1707 relevant tokens.\n",
      "Changing vocabulary for candidate00007. Found 1681 relevant tokens.\n",
      "Reduced to 49 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00005 vs candidate00008...\n",
      "\t Creating author-model for candidate00005 using 1500 features...\n",
      "\t\tfound 7 documents and 9418 relevant tokens.\n",
      "\t Creating author-model for candidate00008 using 1500 features...\n",
      "\t\tfound 7 documents and 9415 relevant tokens.\n",
      "Changing vocabulary for candidate00005. Found 2862 relevant tokens.\n",
      "Changing vocabulary for candidate00008. Found 2785 relevant tokens.\n",
      "Reduced to 77 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00005 vs candidate00009...\n",
      "\t Creating author-model for candidate00005 using 1500 features...\n",
      "\t\tfound 7 documents and 9613 relevant tokens.\n",
      "\t Creating author-model for candidate00009 using 1500 features...\n",
      "\t\tfound 7 documents and 8079 relevant tokens.\n",
      "Changing vocabulary for candidate00005. Found 1833 relevant tokens.\n",
      "Changing vocabulary for candidate00009. Found 1484 relevant tokens.\n",
      "Reduced to 49 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00005 vs candidate00010...\n",
      "\t Creating author-model for candidate00005 using 1500 features...\n",
      "\t\tfound 7 documents and 9553 relevant tokens.\n",
      "\t Creating author-model for candidate00010 using 1500 features...\n",
      "\t\tfound 7 documents and 9257 relevant tokens.\n",
      "Changing vocabulary for candidate00005. Found 1083 relevant tokens.\n",
      "Changing vocabulary for candidate00010. Found 1371 relevant tokens.\n",
      "Reduced to 57 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00006 vs candidate00007...\n",
      "\t Creating author-model for candidate00006 using 1500 features...\n",
      "\t\tfound 7 documents and 8907 relevant tokens.\n",
      "\t Creating author-model for candidate00007 using 1500 features...\n",
      "\t\tfound 7 documents and 8528 relevant tokens.\n",
      "Changing vocabulary for candidate00006. Found 1955 relevant tokens.\n",
      "Changing vocabulary for candidate00007. Found 1714 relevant tokens.\n",
      "Reduced to 31 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00006 vs candidate00008...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Creating author-model for candidate00006 using 1500 features...\n",
      "\t\tfound 7 documents and 8697 relevant tokens.\n",
      "\t Creating author-model for candidate00008 using 1500 features...\n",
      "\t\tfound 7 documents and 9657 relevant tokens.\n",
      "Changing vocabulary for candidate00006. Found 2257 relevant tokens.\n",
      "Changing vocabulary for candidate00008. Found 2182 relevant tokens.\n",
      "Reduced to 61 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00006 vs candidate00009...\n",
      "\t Creating author-model for candidate00006 using 1500 features...\n",
      "\t\tfound 7 documents and 8913 relevant tokens.\n",
      "\t Creating author-model for candidate00009 using 1500 features...\n",
      "\t\tfound 7 documents and 8347 relevant tokens.\n",
      "Changing vocabulary for candidate00006. Found 1358 relevant tokens.\n",
      "Changing vocabulary for candidate00009. Found 1084 relevant tokens.\n",
      "Reduced to 36 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00006 vs candidate00010...\n",
      "\t Creating author-model for candidate00006 using 1500 features...\n",
      "\t\tfound 7 documents and 8810 relevant tokens.\n",
      "\t Creating author-model for candidate00010 using 1500 features...\n",
      "\t\tfound 7 documents and 9534 relevant tokens.\n",
      "Changing vocabulary for candidate00006. Found 738 relevant tokens.\n",
      "Changing vocabulary for candidate00010. Found 988 relevant tokens.\n",
      "Reduced to 34 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00007 vs candidate00008...\n",
      "\t Creating author-model for candidate00007 using 1500 features...\n",
      "\t\tfound 7 documents and 8284 relevant tokens.\n",
      "\t Creating author-model for candidate00008 using 1500 features...\n",
      "\t\tfound 7 documents and 9593 relevant tokens.\n",
      "Changing vocabulary for candidate00007. Found 2352 relevant tokens.\n",
      "Changing vocabulary for candidate00008. Found 2536 relevant tokens.\n",
      "Reduced to 65 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00007 vs candidate00009...\n",
      "\t Creating author-model for candidate00007 using 1500 features...\n",
      "\t\tfound 7 documents and 8519 relevant tokens.\n",
      "\t Creating author-model for candidate00009 using 1500 features...\n",
      "\t\tfound 7 documents and 8306 relevant tokens.\n",
      "Changing vocabulary for candidate00007. Found 2102 relevant tokens.\n",
      "Changing vocabulary for candidate00009. Found 2009 relevant tokens.\n",
      "Reduced to 58 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00007 vs candidate00010...\n",
      "\t Creating author-model for candidate00007 using 1500 features...\n",
      "\t\tfound 7 documents and 8423 relevant tokens.\n",
      "\t Creating author-model for candidate00010 using 1500 features...\n",
      "\t\tfound 7 documents and 9453 relevant tokens.\n",
      "Changing vocabulary for candidate00007. Found 2059 relevant tokens.\n",
      "Changing vocabulary for candidate00010. Found 2403 relevant tokens.\n",
      "Reduced to 50 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00008 vs candidate00009...\n",
      "\t Creating author-model for candidate00008 using 1500 features...\n",
      "\t\tfound 7 documents and 9627 relevant tokens.\n",
      "\t Creating author-model for candidate00009 using 1500 features...\n",
      "\t\tfound 7 documents and 8175 relevant tokens.\n",
      "Changing vocabulary for candidate00008. Found 1939 relevant tokens.\n",
      "Changing vocabulary for candidate00009. Found 1834 relevant tokens.\n",
      "Reduced to 56 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00008 vs candidate00010...\n",
      "\t Creating author-model for candidate00008 using 1500 features...\n",
      "\t\tfound 7 documents and 9538 relevant tokens.\n",
      "\t Creating author-model for candidate00010 using 1500 features...\n",
      "\t\tfound 7 documents and 9258 relevant tokens.\n",
      "Changing vocabulary for candidate00008. Found 1682 relevant tokens.\n",
      "Changing vocabulary for candidate00010. Found 2174 relevant tokens.\n",
      "Reduced to 66 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00009 vs candidate00010...\n",
      "\t Creating author-model for candidate00009 using 1500 features...\n",
      "\t\tfound 7 documents and 8215 relevant tokens.\n",
      "\t Creating author-model for candidate00010 using 1500 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tfound 7 documents and 9502 relevant tokens.\n",
      "Changing vocabulary for candidate00009. Found 1543 relevant tokens.\n",
      "Changing vocabulary for candidate00010. Found 2145 relevant tokens.\n",
      "Reduced to 50 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../HC_aux.py:50: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  z = (uu - ps) / np.sqrt(ps * (1 - ps)) * np.sqrt(n)\n",
      "\n",
      "  2%|▎         | 1/40 [00:00<00:20,  1.89it/s]\u001b[A\n",
      "  5%|▌         | 2/40 [00:00<00:18,  2.06it/s]\u001b[A\n",
      "  8%|▊         | 3/40 [00:01<00:16,  2.31it/s]\u001b[A\n",
      " 10%|█         | 4/40 [00:01<00:14,  2.54it/s]\u001b[A\n",
      " 12%|█▎        | 5/40 [00:01<00:12,  2.74it/s]\u001b[A\n",
      " 15%|█▌        | 6/40 [00:02<00:11,  2.89it/s]\u001b[A\n",
      " 18%|█▊        | 7/40 [00:02<00:10,  3.03it/s]\u001b[A\n",
      " 20%|██        | 8/40 [00:02<00:10,  3.12it/s]\u001b[A\n",
      " 22%|██▎       | 9/40 [00:03<00:09,  3.14it/s]\u001b[A\n",
      " 25%|██▌       | 10/40 [00:03<00:09,  3.21it/s]\u001b[A\n",
      " 28%|██▊       | 11/40 [00:03<00:09,  3.11it/s]\u001b[A\n",
      " 30%|███       | 12/40 [00:03<00:08,  3.15it/s]\u001b[A\n",
      " 32%|███▎      | 13/40 [00:04<00:08,  3.23it/s]\u001b[A\n",
      " 35%|███▌      | 14/40 [00:04<00:08,  3.25it/s]\u001b[A\n",
      " 38%|███▊      | 15/40 [00:04<00:07,  3.17it/s]\u001b[A\n",
      " 40%|████      | 16/40 [00:05<00:07,  3.23it/s]\u001b[A\n",
      " 42%|████▎     | 17/40 [00:05<00:07,  3.22it/s]\u001b[A\n",
      " 45%|████▌     | 18/40 [00:05<00:06,  3.23it/s]\u001b[A\n",
      " 48%|████▊     | 19/40 [00:06<00:06,  3.27it/s]\u001b[A\n",
      " 50%|█████     | 20/40 [00:06<00:06,  3.26it/s]\u001b[A\n",
      " 52%|█████▎    | 21/40 [00:06<00:05,  3.22it/s]\u001b[A\n",
      " 55%|█████▌    | 22/40 [00:07<00:05,  3.26it/s]\u001b[A\n",
      " 57%|█████▊    | 23/40 [00:07<00:05,  3.34it/s]\u001b[A\n",
      " 60%|██████    | 24/40 [00:07<00:05,  3.19it/s]\u001b[A\n",
      " 62%|██████▎   | 25/40 [00:07<00:04,  3.19it/s]\u001b[A\n",
      " 65%|██████▌   | 26/40 [00:08<00:04,  3.20it/s]\u001b[A\n",
      " 68%|██████▊   | 27/40 [00:08<00:03,  3.26it/s]\u001b[A\n",
      " 70%|███████   | 28/40 [00:08<00:03,  3.22it/s]\u001b[A\n",
      " 72%|███████▎  | 29/40 [00:09<00:03,  3.28it/s]\u001b[A\n",
      " 75%|███████▌  | 30/40 [00:09<00:03,  3.29it/s]\u001b[A\n",
      " 78%|███████▊  | 31/40 [00:09<00:02,  3.37it/s]\u001b[A\n",
      " 80%|████████  | 32/40 [00:10<00:02,  3.38it/s]\u001b[A\n",
      " 82%|████████▎ | 33/40 [00:10<00:02,  3.41it/s]\u001b[A\n",
      " 85%|████████▌ | 34/40 [00:10<00:01,  3.43it/s]\u001b[A\n",
      " 88%|████████▊ | 35/40 [00:10<00:01,  3.26it/s]\u001b[A\n",
      " 90%|█████████ | 36/40 [00:11<00:01,  3.27it/s]\u001b[A\n",
      " 92%|█████████▎| 37/40 [00:11<00:00,  3.22it/s]\u001b[A\n",
      " 95%|█████████▌| 38/40 [00:11<00:00,  3.30it/s]\u001b[A\n",
      " 98%|█████████▊| 39/40 [00:12<00:00,  3.31it/s]\u001b[A\n",
      " 50%|█████     | 1/2 [00:22<00:22, 22.78s/it]s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem = PAN-problem00003\n",
      "recall = 1.0\n",
      "accuracy = 0.7\n",
      "F1 = 0.8235294117647058\n",
      "Found 10 author-pairs\n",
      "MultiBinaryAuthorModel: Creating model for candidate00001 vs candidate00002...\n",
      "\t Creating author-model for candidate00001 using 1500 features...\n",
      "\t\tfound 7 documents and 8389 relevant tokens.\n",
      "\t Creating author-model for candidate00002 using 1500 features...\n",
      "\t\tfound 7 documents and 8290 relevant tokens.\n",
      "Changing vocabulary for candidate00001. Found 2720 relevant tokens.\n",
      "Changing vocabulary for candidate00002. Found 2901 relevant tokens.\n",
      "Reduced to 417 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00001 vs candidate00003...\n",
      "\t Creating author-model for candidate00001 using 1500 features...\n",
      "\t\tfound 7 documents and 8302 relevant tokens.\n",
      "\t Creating author-model for candidate00003 using 1500 features...\n",
      "\t\tfound 7 documents and 9295 relevant tokens.\n",
      "Changing vocabulary for candidate00001. Found 1592 relevant tokens.\n",
      "Changing vocabulary for candidate00003. Found 2073 relevant tokens.\n",
      "Reduced to 58 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00001 vs candidate00004...\n",
      "\t Creating author-model for candidate00001 using 1500 features...\n",
      "\t\tfound 7 documents and 8332 relevant tokens.\n",
      "\t Creating author-model for candidate00004 using 1500 features...\n",
      "\t\tfound 7 documents and 7427 relevant tokens.\n",
      "Changing vocabulary for candidate00001. Found 1576 relevant tokens.\n",
      "Changing vocabulary for candidate00004. Found 1729 relevant tokens.\n",
      "Reduced to 49 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00001 vs candidate00005...\n",
      "\t Creating author-model for candidate00001 using 1500 features...\n",
      "\t\tfound 7 documents and 8311 relevant tokens.\n",
      "\t Creating author-model for candidate00005 using 1500 features...\n",
      "\t\tfound 7 documents and 7763 relevant tokens.\n",
      "Changing vocabulary for candidate00001. Found 2238 relevant tokens.\n",
      "Changing vocabulary for candidate00005. Found 2446 relevant tokens.\n",
      "Reduced to 249 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00002 vs candidate00003...\n",
      "\t Creating author-model for candidate00002 using 1500 features...\n",
      "\t\tfound 7 documents and 8280 relevant tokens.\n",
      "\t Creating author-model for candidate00003 using 1500 features...\n",
      "\t\tfound 7 documents and 9355 relevant tokens.\n",
      "Changing vocabulary for candidate00002. Found 1572 relevant tokens.\n",
      "Changing vocabulary for candidate00003. Found 1990 relevant tokens.\n",
      "Reduced to 34 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00002 vs candidate00004...\n",
      "\t Creating author-model for candidate00002 using 1500 features...\n",
      "\t\tfound 7 documents and 8276 relevant tokens.\n",
      "\t Creating author-model for candidate00004 using 1500 features...\n",
      "\t\tfound 7 documents and 7427 relevant tokens.\n",
      "Changing vocabulary for candidate00002. Found 3662 relevant tokens.\n",
      "Changing vocabulary for candidate00004. Found 3511 relevant tokens.\n",
      "Reduced to 459 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00002 vs candidate00005...\n",
      "\t Creating author-model for candidate00002 using 1500 features...\n",
      "\t\tfound 7 documents and 8259 relevant tokens.\n",
      "\t Creating author-model for candidate00005 using 1500 features...\n",
      "\t\tfound 7 documents and 7763 relevant tokens.\n",
      "Changing vocabulary for candidate00002. Found 2724 relevant tokens.\n",
      "Changing vocabulary for candidate00005. Found 2841 relevant tokens.\n",
      "Reduced to 502 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00003 vs candidate00004...\n",
      "\t Creating author-model for candidate00003 using 1500 features...\n",
      "\t\tfound 7 documents and 9387 relevant tokens.\n",
      "\t Creating author-model for candidate00004 using 1500 features...\n",
      "\t\tfound 7 documents and 7502 relevant tokens.\n",
      "Changing vocabulary for candidate00003. Found 5138 relevant tokens.\n",
      "Changing vocabulary for candidate00004. Found 4322 relevant tokens.\n",
      "Reduced to 452 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00003 vs candidate00005...\n",
      "\t Creating author-model for candidate00003 using 1500 features...\n",
      "\t\tfound 7 documents and 9354 relevant tokens.\n",
      "\t Creating author-model for candidate00005 using 1500 features...\n",
      "\t\tfound 7 documents and 7784 relevant tokens.\n",
      "Changing vocabulary for candidate00003. Found 3274 relevant tokens.\n",
      "Changing vocabulary for candidate00005. Found 2922 relevant tokens.\n",
      "Reduced to 321 features...\n",
      "MultiBinaryAuthorModel: Creating model for candidate00004 vs candidate00005...\n",
      "\t Creating author-model for candidate00004 using 1500 features...\n",
      "\t\tfound 7 documents and 7492 relevant tokens.\n",
      "\t Creating author-model for candidate00005 using 1500 features...\n",
      "\t\tfound 7 documents and 7789 relevant tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing vocabulary for candidate00004. Found 4442 relevant tokens.\n",
      "Changing vocabulary for candidate00005. Found 4485 relevant tokens.\n",
      "Reduced to 548 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 12%|█▎        | 2/16 [00:00<00:01, 12.53it/s]\u001b[A\n",
      " 25%|██▌       | 4/16 [00:00<00:00, 12.52it/s]\u001b[A\n",
      " 38%|███▊      | 6/16 [00:00<00:00, 12.81it/s]\u001b[A\n",
      " 50%|█████     | 8/16 [00:00<00:00, 13.03it/s]\u001b[A\n",
      " 62%|██████▎   | 10/16 [00:00<00:00, 13.35it/s]\u001b[A\n",
      " 75%|███████▌  | 12/16 [00:00<00:00, 13.33it/s]\u001b[A\n",
      " 88%|████████▊ | 14/16 [00:01<00:00, 13.45it/s]\u001b[A\n",
      "100%|██████████| 2/2 [00:27<00:00, 17.24s/it]s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem = PAN-problem00004\n",
      "recall = 1.0\n",
      "accuracy = 0.625\n",
      "F1 = 0.7692307692307693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lo_problem = pd.unique(data['dataset'])\n",
    "\n",
    "lo_F1 = []\n",
    "lo_acc = []\n",
    "\n",
    "for prob in tqdm(lo_problem[2:4]) :\n",
    "    data_prob = data[data['dataset'] == prob]\n",
    "    data_train = data_prob[data_prob['split'] == 'train']\n",
    "    \n",
    "    #compute model for each problem:\n",
    "    model = AuthorshipAttributionMultiBinary(data_train, \n",
    "                                       vocab_size = 1500,  #uses most frequent ngram\n",
    "                                       stbl = True,  #type of HC statistic\n",
    "                                      ngram_range = (1,3), #mono-, bi-, and tri- grams\n",
    "                                         reduce_features=True,\n",
    "                                         randomize=True\n",
    "                                                )\n",
    "    \n",
    "    #attribute test documents:\n",
    "    data_test = data_prob[data_prob['split'] == 'test']\n",
    "    lo_test_docs = pd.unique(data_test.doc_id)\n",
    "    df = pd.DataFrame() #save results in this dataframe\n",
    "\n",
    "    for doc in tqdm(lo_test_docs) :\n",
    "        sm = data_test[data_test.doc_id == doc]\n",
    "\n",
    "        pred = model.predict(sm.text.values[0], method = 'chisq_pval', LOO = False) \n",
    "                # can use 'unk_thresh' to get '<UNK>' instead of the name \n",
    "                # of the corpus with smallest HC in the case when the smallest\n",
    "                # HC is above 'unk_thresh'. \n",
    "\n",
    "        auth = sm.author.values[0]\n",
    "        df = df.append({'doc_id' : doc,\n",
    "                   'author' : auth,\n",
    "                   'predicted' : pred,\n",
    "                  }, ignore_index = True)\n",
    "\n",
    "\n",
    "    # evaluate accuracy and F1 score\n",
    "    df_r = df[df.predicted != '<UNK>']\n",
    "    recall = len(df_r) / len(df)\n",
    "    acc = np.mean((df_r.predicted == df_r.author).values)\n",
    "    \n",
    "    print(\"problem = {}\".format(prob))\n",
    "    print(\"recall = {}\".format(recall))\n",
    "    print(\"accuracy = {}\".format(acc))\n",
    "    print(\"F1 = {}\".format(2*recall*acc / (recall + acc)))\n",
    "    lo_F1 += [2*recall*acc / (recall + acc)]\n",
    "    lo_acc += [acc]\n",
    "\n",
    "#prob4: F1 = 0.72, acc = 0.5625, |W| = 100, ng = (1,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8048830385743897"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(lo_F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Other Classifyers</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size = 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 1/4 [00:00<00:02,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "using <class 'sklearn.svm.classes.LinearSVC'>\n",
      "using <class 'sklearn.neighbors.classification.KNeighborsClassifier'>\n",
      "vocab size = 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 2/4 [00:01<00:01,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "using <class 'sklearn.svm.classes.LinearSVC'>\n",
      "using <class 'sklearn.neighbors.classification.KNeighborsClassifier'>\n",
      "vocab size = 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 3/4 [00:02<00:00,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "using <class 'sklearn.svm.classes.LinearSVC'>\n",
      "using <class 'sklearn.neighbors.classification.KNeighborsClassifier'>\n",
      "vocab size = 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 4/4 [00:02<00:00,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "using <class 'sklearn.svm.classes.LinearSVC'>\n",
      "using <class 'sklearn.neighbors.classification.KNeighborsClassifier'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "multinomial_NB    0.822121\n",
       "SVM               0.773446\n",
       "KNN               0.719383\n",
       "vocab_size        1.999334\n",
       "dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import nltk\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import sys\n",
    "\n",
    "\n",
    "from AuthAttLib import AuthorshipAttributionMulti, to_docTermCounts\n",
    "from FreqTable import FreqTable\n",
    "\n",
    "def evaluate_classifyer(Cls, X_train, y_train, X_test, y_test, **kwargs) :\n",
    "    clf = Cls(**kwargs)\n",
    "    clf.fit(X_train,y_train)\n",
    "    return clf.score(X_test, y_test)\n",
    "\n",
    "def evaluate_Clf(Cls, X_train, y_train,\n",
    "                 X_test, y_test,\n",
    "                 vocab, **kwargs) :\n",
    "    print('using {}'.format(Cls))\n",
    "    acc = []\n",
    "    clf = Cls(**kwargs)\n",
    "\n",
    "    clf.fit(X_train,y_train)\n",
    "    acc = clf.score(X_test, y_test)\n",
    "    return acc\n",
    "\n",
    "    clf.fit(X_train,y_train)\n",
    "    return clf.score(X_test, y_test)\n",
    "\n",
    "def get_n_most_common_words(n = 5000) :\n",
    "    most_common_list = pd.read_csv('~/Data/5000_most_common_english_words.csv')\n",
    "    return list(set(most_common_list.Word.tolist()))[:n]\n",
    "\n",
    "def get_counts_labels(df, vocab) :\n",
    "#prepare data:\n",
    "    X = []\n",
    "    y = []\n",
    "    for r in df.iterrows() :\n",
    "        dt = to_docTermCounts([r[1].text], \n",
    "                            vocab=vocab\n",
    "                             )\n",
    "        X += [FreqTable(dt[0], dt[1])._counts]\n",
    "        y += [r[1].author]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "classifiers = {#'logistic_regression' : LogisticRegression,\n",
    "                'multinomial_NB' : MultinomialNB,\n",
    "                 'SVM' : LinearSVC,\n",
    "                 'KNN' : KNeighborsClassifier,\n",
    "                  }\n",
    "\n",
    "lo_args = {'logistic_regression' : {'solver': 'lbfgs',\n",
    "                                   'max_iter' : 150},\n",
    "            'multinomial_NB' : {},\n",
    "                 'SVM' : {'dual' : False},\n",
    "                 'KNN' : {'metric' : 'cosine',\n",
    "                          'n_neighbors' : 5},\n",
    "            }\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "lo_problem = pd.unique(data['prob'])\n",
    "\n",
    "for prob in tqdm(lo_problem[:4]) :\n",
    "    data_prob = data[data['prob'] == prob]\n",
    "    vocab = model._vocab\n",
    "    print(\"vocab size = {}\".format(vocab_size))\n",
    "    acc = {}\n",
    "\n",
    "    \n",
    "    X_train, y_train = get_counts_labels(data_prob[data_prob.split=='train'], vocab)\n",
    "    X_test, y_test = get_counts_labels(data_prob[data_prob.split=='test'], vocab)\n",
    "\n",
    "    for cls in classifiers : \n",
    "        Cls = classifiers[cls]\n",
    "        args = lo_args[cls]\n",
    "        acc[cls] = evaluate_Clf(Cls, X_train,y_train, X_test, y_test, vocab, **args)\n",
    "\n",
    "    acc['vocab_size'] = vocab_size\n",
    "\n",
    "    df = df.append(pd.DataFrame(acc, index = [0]), ignore_index = True)\n",
    "    \n",
    "df.apply(lambda x : 2*x/(1+x)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
