{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>HC in Cross-Domain Authorship Attribution Challenge</H1>\n",
    "Use this notebook to replicate the results reporter in the paper  </br>\n",
    "<ul>\n",
    "    [1] <a href = https://arxiv.org/abs/1911.01208>\n",
    "    Kipnis, A., ``Higher Criticism for Discriminating Word-Frequency Tables and Testing Authorship'', 2019\n",
    "    </a>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use HC-based test to attribute authorship in the PAN2018 cross-domain authorship attribution challenge https://pan.webis.de/clef18/pan18-web/author-identification.html#cross-domain\n",
    "- Only the English part (problems 1-4) of this challenge is considered. \n",
    "- We use a lemmatized version of the data obtained using the Stanford CoreNLP (https://stanfordnlp.github.io/CoreNLP/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import codecs\n",
    "from tqdm import tqdm\n",
    "\n",
    "#import auxiliary functions for python\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from AuthAttLib import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>Load Data</H2>\n",
    "The data below was obtained by lemmatizing the original challenge test data using the Stanford CoreNLP lemmatizer https://stanfordnlp.github.io/CoreNLP/. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data PAN2018 (from lemmatized) \n",
    "raw_data = pd.read_csv(\"./Data/PAN2018_lemmatized.csv\")\n",
    "raw_data.loc[:,'lem_text'] = raw_data.loc[:,'lem_text'] # + ' ' + raw_data.loc[:,'POS']\n",
    "data = raw_data.filter(['dataset', 'author', 'doc_id'])\n",
    "\n",
    "data.loc[:,'split'] = 'train'\n",
    "data.loc[data.doc_id.str.find('test')>-1,'split'] = 'test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore proper names, numbers, and some pronouns: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove proper names\n",
    "data.loc[:,'text'] = raw_data.apply(\n",
    "    lambda r : \" \".join([w[0] for w in zip(r['lem_text'].split(),r['POS'].split()) if w[1] != 'PROPN']), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>Multi-author model</H2>\n",
    "\n",
    "- For each problem in the challenge, train a model and evaluate it over a test set. <br>\n",
    "- The following implementation opt not to use the UNNKOWN option, hence the recall is always 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Creating author-model for candidate00001 using 1500 features...\n",
      "\t\tfound 7 documents and 8588 relevant tokens.\n",
      "\t Creating author-model for candidate00002 using 1500 features...\n",
      "\t\tfound 7 documents and 8498 relevant tokens.\n",
      "\t Creating author-model for candidate00003 using 1500 features...\n",
      "\t\tfound 7 documents and 7619 relevant tokens.\n",
      "\t Creating author-model for candidate00004 using 1500 features...\n",
      "\t\tfound 7 documents and 8383 relevant tokens.\n",
      "\t Creating author-model for candidate00005 using 1500 features...\n",
      "\t\tfound 7 documents and 7809 relevant tokens.\n",
      "\t Creating author-model for candidate00006 using 1500 features...\n",
      "\t\tfound 7 documents and 8443 relevant tokens.\n",
      "\t Creating author-model for candidate00007 using 1500 features...\n",
      "\t\tfound 7 documents and 7806 relevant tokens.\n",
      "\t Creating author-model for candidate00008 using 1500 features...\n",
      "\t\tfound 7 documents and 7433 relevant tokens.\n",
      "\t Creating author-model for candidate00009 using 1500 features...\n",
      "\t\tfound 7 documents and 7846 relevant tokens.\n",
      "\t Creating author-model for candidate00010 using 1500 features...\n",
      "\t\tfound 7 documents and 8842 relevant tokens.\n",
      "\t Creating author-model for candidate00011 using 1500 features...\n",
      "\t\tfound 7 documents and 7213 relevant tokens.\n",
      "\t Creating author-model for candidate00012 using 1500 features...\n",
      "\t\tfound 7 documents and 8085 relevant tokens.\n",
      "\t Creating author-model for candidate00013 using 1500 features...\n",
      "\t\tfound 7 documents and 7698 relevant tokens.\n",
      "\t Creating author-model for candidate00014 using 1500 features...\n",
      "\t\tfound 7 documents and 8217 relevant tokens.\n",
      "\t Creating author-model for candidate00015 using 1500 features...\n",
      "\t\tfound 7 documents and 8421 relevant tokens.\n",
      "\t Creating author-model for candidate00016 using 1500 features...\n",
      "\t\tfound 7 documents and 7071 relevant tokens.\n",
      "\t Creating author-model for candidate00017 using 1500 features...\n",
      "\t\tfound 7 documents and 6825 relevant tokens.\n",
      "\t Creating author-model for candidate00018 using 1500 features...\n",
      "\t\tfound 7 documents and 8445 relevant tokens.\n",
      "\t Creating author-model for candidate00019 using 1500 features...\n",
      "\t\tfound 7 documents and 8658 relevant tokens.\n",
      "\t Creating author-model for candidate00020 using 1500 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/79 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tfound 7 documents and 8249 relevant tokens.\n",
      "Evaluate on test dataset:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|▏         | 1/79 [00:06<08:36,  6.62s/it]\u001b[A\n",
      "  3%|▎         | 2/79 [00:13<08:24,  6.55s/it]\u001b[A\n",
      "  4%|▍         | 3/79 [00:19<08:21,  6.60s/it]\u001b[A\n",
      "  5%|▌         | 4/79 [00:26<08:08,  6.52s/it]\u001b[A\n",
      "  6%|▋         | 5/79 [00:32<08:04,  6.55s/it]\u001b[A\n",
      "  8%|▊         | 6/79 [00:41<08:37,  7.10s/it]\u001b[A\n",
      "  9%|▉         | 7/79 [00:47<08:23,  6.99s/it]\u001b[A\n",
      " 10%|█         | 8/79 [00:55<08:23,  7.09s/it]\u001b[A\n",
      " 11%|█▏        | 9/79 [01:03<08:36,  7.38s/it]\u001b[A\n",
      " 13%|█▎        | 10/79 [01:10<08:29,  7.39s/it]\u001b[A\n",
      " 14%|█▍        | 11/79 [01:18<08:23,  7.40s/it]\u001b[A\n",
      " 15%|█▌        | 12/79 [01:24<08:05,  7.25s/it]\u001b[A\n",
      " 16%|█▋        | 13/79 [01:31<07:54,  7.19s/it]\u001b[A\n",
      " 18%|█▊        | 14/79 [01:38<07:36,  7.03s/it]\u001b[A\n",
      " 19%|█▉        | 15/79 [01:45<07:31,  7.06s/it]\u001b[A\n",
      " 20%|██        | 16/79 [01:53<07:29,  7.13s/it]\u001b[A\n",
      " 22%|██▏       | 17/79 [02:00<07:24,  7.16s/it]\u001b[A\n",
      " 23%|██▎       | 18/79 [02:07<07:18,  7.20s/it]\u001b[A\n",
      " 24%|██▍       | 19/79 [02:14<07:08,  7.15s/it]\u001b[A\n",
      " 25%|██▌       | 20/79 [02:21<07:00,  7.13s/it]\u001b[A\n",
      " 27%|██▋       | 21/79 [02:28<06:53,  7.13s/it]\u001b[A\n",
      " 28%|██▊       | 22/79 [02:35<06:46,  7.13s/it]\u001b[A\n",
      " 29%|██▉       | 23/79 [02:42<06:34,  7.05s/it]\u001b[A\n",
      " 30%|███       | 24/79 [02:49<06:28,  7.06s/it]\u001b[A\n",
      " 32%|███▏      | 25/79 [02:56<06:12,  6.91s/it]\u001b[A\n",
      " 33%|███▎      | 26/79 [03:03<06:00,  6.81s/it]\u001b[A\n",
      " 34%|███▍      | 27/79 [03:09<05:52,  6.77s/it]\u001b[A\n",
      " 35%|███▌      | 28/79 [03:16<05:48,  6.84s/it]\u001b[A\n",
      " 37%|███▋      | 29/79 [03:23<05:40,  6.81s/it]\u001b[A\n",
      " 38%|███▊      | 30/79 [03:30<05:34,  6.83s/it]\u001b[A\n",
      " 39%|███▉      | 31/79 [03:37<05:29,  6.86s/it]\u001b[A\n",
      " 41%|████      | 32/79 [03:44<05:21,  6.84s/it]\u001b[A\n",
      " 42%|████▏     | 33/79 [03:51<05:19,  6.94s/it]\u001b[A\n",
      " 43%|████▎     | 34/79 [03:57<05:07,  6.83s/it]\u001b[A\n",
      " 44%|████▍     | 35/79 [04:04<04:57,  6.75s/it]\u001b[A\n",
      " 46%|████▌     | 36/79 [04:11<04:51,  6.78s/it]\u001b[A\n",
      " 47%|████▋     | 37/79 [04:18<04:55,  7.03s/it]\u001b[A\n",
      " 48%|████▊     | 38/79 [04:25<04:47,  7.01s/it]\u001b[A\n",
      " 49%|████▉     | 39/79 [04:32<04:41,  7.05s/it]\u001b[A\n",
      " 51%|█████     | 40/79 [04:40<04:39,  7.16s/it]\u001b[A\n",
      " 52%|█████▏    | 41/79 [04:47<04:30,  7.12s/it]\u001b[A\n",
      " 53%|█████▎    | 42/79 [04:54<04:23,  7.12s/it]\u001b[A\n",
      " 54%|█████▍    | 43/79 [05:01<04:13,  7.04s/it]\u001b[A\n",
      " 56%|█████▌    | 44/79 [05:08<04:11,  7.18s/it]\u001b[A\n",
      " 57%|█████▋    | 45/79 [05:15<04:01,  7.10s/it]\u001b[A\n",
      " 58%|█████▊    | 46/79 [05:23<03:57,  7.19s/it]\u001b[A\n",
      " 59%|█████▉    | 47/79 [05:30<03:48,  7.15s/it]\u001b[A\n",
      " 61%|██████    | 48/79 [05:37<03:39,  7.07s/it]\u001b[A\n",
      " 62%|██████▏   | 49/79 [05:44<03:30,  7.02s/it]\u001b[A\n",
      " 63%|██████▎   | 50/79 [05:50<03:21,  6.95s/it]\u001b[A\n",
      " 65%|██████▍   | 51/79 [05:57<03:11,  6.85s/it]\u001b[A\n",
      " 66%|██████▌   | 52/79 [06:04<03:03,  6.78s/it]\u001b[A\n",
      " 67%|██████▋   | 53/79 [06:10<02:56,  6.80s/it]\u001b[A\n",
      " 68%|██████▊   | 54/79 [06:17<02:50,  6.83s/it]\u001b[A\n",
      " 70%|██████▉   | 55/79 [06:24<02:44,  6.84s/it]\u001b[A\n",
      " 71%|███████   | 56/79 [06:31<02:36,  6.81s/it]\u001b[A\n",
      " 72%|███████▏  | 57/79 [06:38<02:31,  6.87s/it]\u001b[A\n",
      " 73%|███████▎  | 58/79 [06:45<02:25,  6.93s/it]\u001b[A\n",
      " 75%|███████▍  | 59/79 [06:52<02:19,  6.97s/it]\u001b[A\n",
      " 76%|███████▌  | 60/79 [06:59<02:11,  6.93s/it]\u001b[A\n",
      " 77%|███████▋  | 61/79 [07:06<02:05,  6.96s/it]\u001b[A\n",
      " 78%|███████▊  | 62/79 [07:13<01:58,  6.95s/it]\u001b[A\n",
      " 80%|███████▉  | 63/79 [07:20<01:52,  7.01s/it]\u001b[A\n",
      " 81%|████████  | 64/79 [07:27<01:45,  7.05s/it]\u001b[A\n",
      " 82%|████████▏ | 65/79 [07:34<01:38,  7.02s/it]\u001b[A\n",
      " 84%|████████▎ | 66/79 [07:41<01:32,  7.12s/it]\u001b[A\n",
      " 85%|████████▍ | 67/79 [07:48<01:25,  7.11s/it]\u001b[A\n",
      " 86%|████████▌ | 68/79 [07:56<01:18,  7.17s/it]\u001b[A\n",
      " 87%|████████▋ | 69/79 [08:03<01:11,  7.14s/it]\u001b[A\n",
      " 89%|████████▊ | 70/79 [08:10<01:03,  7.05s/it]\u001b[A\n",
      " 90%|████████▉ | 71/79 [08:17<00:57,  7.25s/it]\u001b[A\n",
      " 91%|█████████ | 72/79 [08:25<00:51,  7.36s/it]\u001b[A\n",
      " 92%|█████████▏| 73/79 [08:32<00:44,  7.39s/it]\u001b[A\n",
      " 94%|█████████▎| 74/79 [08:39<00:36,  7.23s/it]\u001b[A\n",
      " 95%|█████████▍| 75/79 [08:46<00:28,  7.18s/it]\u001b[A\n",
      " 96%|█████████▌| 76/79 [08:54<00:21,  7.29s/it]\u001b[A\n",
      " 97%|█████████▋| 77/79 [09:01<00:14,  7.24s/it]\u001b[A\n",
      " 99%|█████████▊| 78/79 [09:09<00:07,  7.32s/it]\u001b[A\n",
      " 25%|██▌       | 1/4 [09:20<28:01, 560.59s/it]]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem = PAN-problem00001\n",
      "recall = 1.0\n",
      "accuracy = 0.5063291139240507\n",
      "F1 = 0.6722689075630253\n",
      "\t Creating author-model for candidate00001 using 1500 features...\n",
      "\t\tfound 7 documents and 8852 relevant tokens.\n",
      "\t Creating author-model for candidate00002 using 1500 features...\n",
      "\t\tfound 7 documents and 6824 relevant tokens.\n",
      "\t Creating author-model for candidate00003 using 1500 features...\n",
      "\t\tfound 7 documents and 7603 relevant tokens.\n",
      "\t Creating author-model for candidate00004 using 1500 features...\n",
      "\t\tfound 7 documents and 8565 relevant tokens.\n",
      "\t Creating author-model for candidate00005 using 1500 features...\n",
      "\t\tfound 7 documents and 7114 relevant tokens.\n",
      "\t Creating author-model for candidate00006 using 1500 features...\n",
      "\t\tfound 7 documents and 8233 relevant tokens.\n",
      "\t Creating author-model for candidate00007 using 1500 features...\n",
      "\t\tfound 7 documents and 8456 relevant tokens.\n",
      "\t Creating author-model for candidate00008 using 1500 features...\n",
      "\t\tfound 7 documents and 8445 relevant tokens.\n",
      "\t Creating author-model for candidate00009 using 1500 features...\n",
      "\t\tfound 7 documents and 7251 relevant tokens.\n",
      "\t Creating author-model for candidate00010 using 1500 features...\n",
      "\t\tfound 7 documents and 7751 relevant tokens.\n",
      "\t Creating author-model for candidate00011 using 1500 features...\n",
      "\t\tfound 7 documents and 7865 relevant tokens.\n",
      "\t Creating author-model for candidate00012 using 1500 features...\n",
      "\t\tfound 7 documents and 8108 relevant tokens.\n",
      "\t Creating author-model for candidate00013 using 1500 features...\n",
      "\t\tfound 7 documents and 8276 relevant tokens.\n",
      "\t Creating author-model for candidate00014 using 1500 features...\n",
      "\t\tfound 7 documents and 7823 relevant tokens.\n",
      "\t Creating author-model for candidate00015 using 1500 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/74 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tfound 7 documents and 8665 relevant tokens.\n",
      "Evaluate on test dataset:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|▏         | 1/74 [00:05<06:06,  5.02s/it]\u001b[A\n",
      "  3%|▎         | 2/74 [00:10<06:04,  5.06s/it]\u001b[A\n",
      "  4%|▍         | 3/74 [00:15<06:00,  5.08s/it]\u001b[A\n",
      "  5%|▌         | 4/74 [00:20<05:59,  5.13s/it]\u001b[A\n",
      "  7%|▋         | 5/74 [00:25<05:53,  5.13s/it]\u001b[A\n",
      "  8%|▊         | 6/74 [00:30<05:50,  5.16s/it]\u001b[A\n",
      "  9%|▉         | 7/74 [00:36<05:45,  5.16s/it]\u001b[A\n",
      " 11%|█         | 8/74 [00:41<05:44,  5.21s/it]\u001b[A\n",
      " 12%|█▏        | 9/74 [00:47<05:46,  5.34s/it]\u001b[A\n",
      " 14%|█▎        | 10/74 [00:52<05:39,  5.31s/it]\u001b[A\n",
      " 15%|█▍        | 11/74 [00:57<05:29,  5.23s/it]\u001b[A\n",
      " 16%|█▌        | 12/74 [01:02<05:21,  5.19s/it]\u001b[A\n",
      " 18%|█▊        | 13/74 [01:07<05:10,  5.09s/it]\u001b[A\n",
      " 19%|█▉        | 14/74 [01:12<05:08,  5.14s/it]\u001b[A\n",
      " 20%|██        | 15/74 [01:17<05:03,  5.15s/it]\u001b[A\n",
      " 22%|██▏       | 16/74 [01:22<04:57,  5.13s/it]\u001b[A\n",
      " 23%|██▎       | 17/74 [01:27<04:53,  5.15s/it]\u001b[A\n",
      " 24%|██▍       | 18/74 [01:33<04:49,  5.17s/it]\u001b[A\n",
      " 26%|██▌       | 19/74 [01:38<04:46,  5.22s/it]\u001b[A\n",
      " 27%|██▋       | 20/74 [01:44<04:53,  5.44s/it]\u001b[A\n",
      " 28%|██▊       | 21/74 [01:49<04:46,  5.40s/it]\u001b[A\n",
      " 30%|██▉       | 22/74 [01:55<04:40,  5.40s/it]\u001b[A\n",
      " 31%|███       | 23/74 [02:00<04:29,  5.28s/it]\u001b[A\n",
      " 32%|███▏      | 24/74 [02:05<04:26,  5.33s/it]\u001b[A\n",
      " 34%|███▍      | 25/74 [02:10<04:15,  5.21s/it]\u001b[A\n",
      " 35%|███▌      | 26/74 [02:15<04:06,  5.14s/it]\u001b[A\n",
      " 36%|███▋      | 27/74 [02:21<04:13,  5.40s/it]\u001b[A\n",
      " 38%|███▊      | 28/74 [02:27<04:11,  5.46s/it]\u001b[A\n",
      " 39%|███▉      | 29/74 [02:32<04:05,  5.46s/it]\u001b[A\n",
      " 41%|████      | 30/74 [02:37<03:58,  5.42s/it]\u001b[A\n",
      " 42%|████▏     | 31/74 [02:43<03:54,  5.45s/it]\u001b[A\n",
      " 43%|████▎     | 32/74 [02:49<03:52,  5.53s/it]\u001b[A\n",
      " 45%|████▍     | 33/74 [02:54<03:39,  5.35s/it]\u001b[A\n",
      " 46%|████▌     | 34/74 [02:59<03:29,  5.23s/it]\u001b[A\n",
      " 47%|████▋     | 35/74 [03:04<03:23,  5.22s/it]\u001b[A\n",
      " 49%|████▊     | 36/74 [03:09<03:17,  5.19s/it]\u001b[A\n",
      " 50%|█████     | 37/74 [03:15<03:18,  5.37s/it]\u001b[A\n",
      " 51%|█████▏    | 38/74 [03:20<03:16,  5.47s/it]\u001b[A\n",
      " 53%|█████▎    | 39/74 [03:25<03:07,  5.36s/it]\u001b[A\n",
      " 54%|█████▍    | 40/74 [03:31<03:00,  5.29s/it]\u001b[A\n",
      " 55%|█████▌    | 41/74 [03:35<02:49,  5.14s/it]\u001b[A\n",
      " 57%|█████▋    | 42/74 [03:40<02:43,  5.10s/it]\u001b[A\n",
      " 58%|█████▊    | 43/74 [03:45<02:37,  5.08s/it]\u001b[A\n",
      " 59%|█████▉    | 44/74 [03:50<02:30,  5.03s/it]\u001b[A\n",
      " 61%|██████    | 45/74 [03:55<02:25,  5.03s/it]\u001b[A\n",
      " 62%|██████▏   | 46/74 [04:01<02:23,  5.12s/it]\u001b[A\n",
      " 64%|██████▎   | 47/74 [04:06<02:18,  5.12s/it]\u001b[A\n",
      " 65%|██████▍   | 48/74 [04:11<02:12,  5.08s/it]\u001b[A\n",
      " 66%|██████▌   | 49/74 [04:16<02:05,  5.03s/it]\u001b[A\n",
      " 68%|██████▊   | 50/74 [04:21<02:00,  5.03s/it]\u001b[A\n",
      " 69%|██████▉   | 51/74 [04:26<01:54,  4.96s/it]\u001b[A\n",
      " 70%|███████   | 52/74 [04:31<01:52,  5.10s/it]\u001b[A\n",
      " 72%|███████▏  | 53/74 [04:37<01:53,  5.39s/it]\u001b[A\n",
      " 73%|███████▎  | 54/74 [04:43<01:50,  5.50s/it]\u001b[A\n",
      " 74%|███████▍  | 55/74 [04:48<01:45,  5.55s/it]\u001b[A\n",
      " 76%|███████▌  | 56/74 [04:54<01:38,  5.45s/it]\u001b[A\n",
      " 77%|███████▋  | 57/74 [04:59<01:32,  5.45s/it]\u001b[A\n",
      " 78%|███████▊  | 58/74 [05:04<01:24,  5.28s/it]\u001b[A\n",
      " 80%|███████▉  | 59/74 [05:09<01:17,  5.20s/it]\u001b[A\n",
      " 81%|████████  | 60/74 [05:14<01:12,  5.17s/it]\u001b[A\n",
      " 82%|████████▏ | 61/74 [05:19<01:06,  5.13s/it]\u001b[A\n",
      " 84%|████████▍ | 62/74 [05:24<01:01,  5.10s/it]\u001b[A\n",
      " 85%|████████▌ | 63/74 [05:30<00:57,  5.27s/it]\u001b[A\n",
      " 86%|████████▋ | 64/74 [05:35<00:53,  5.37s/it]\u001b[A\n",
      " 88%|████████▊ | 65/74 [05:40<00:47,  5.26s/it]\u001b[A\n",
      " 89%|████████▉ | 66/74 [05:45<00:41,  5.15s/it]\u001b[A\n",
      " 91%|█████████ | 67/74 [05:50<00:35,  5.11s/it]\u001b[A\n",
      " 92%|█████████▏| 68/74 [05:55<00:30,  5.03s/it]\u001b[A\n",
      " 93%|█████████▎| 69/74 [06:00<00:25,  5.02s/it]\u001b[A\n",
      " 95%|█████████▍| 70/74 [06:05<00:19,  4.99s/it]\u001b[A\n",
      " 96%|█████████▌| 71/74 [06:10<00:15,  5.02s/it]\u001b[A\n",
      " 97%|█████████▋| 72/74 [06:15<00:10,  5.02s/it]\u001b[A\n",
      " 99%|█████████▊| 73/74 [06:20<00:05,  5.04s/it]\u001b[A\n",
      " 50%|█████     | 2/4 [15:49<16:58, 509.09s/it]]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem = PAN-problem00002\n",
      "recall = 1.0\n",
      "accuracy = 0.5135135135135135\n",
      "F1 = 0.6785714285714285\n",
      "\t Creating author-model for candidate00001 using 1500 features...\n",
      "\t\tfound 7 documents and 7296 relevant tokens.\n",
      "\t Creating author-model for candidate00002 using 1500 features...\n",
      "\t\tfound 7 documents and 8874 relevant tokens.\n",
      "\t Creating author-model for candidate00003 using 1500 features...\n",
      "\t\tfound 7 documents and 6842 relevant tokens.\n",
      "\t Creating author-model for candidate00004 using 1500 features...\n",
      "\t\tfound 7 documents and 8715 relevant tokens.\n",
      "\t Creating author-model for candidate00005 using 1500 features...\n",
      "\t\tfound 7 documents and 8562 relevant tokens.\n",
      "\t Creating author-model for candidate00006 using 1500 features...\n",
      "\t\tfound 7 documents and 8347 relevant tokens.\n",
      "\t Creating author-model for candidate00007 using 1500 features...\n",
      "\t\tfound 7 documents and 7874 relevant tokens.\n",
      "\t Creating author-model for candidate00008 using 1500 features...\n",
      "\t\tfound 7 documents and 8688 relevant tokens.\n",
      "\t Creating author-model for candidate00009 using 1500 features...\n",
      "\t\tfound 7 documents and 7757 relevant tokens.\n",
      "\t Creating author-model for candidate00010 using 1500 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tfound 7 documents and 8751 relevant tokens.\n",
      "Evaluate on test dataset:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▎         | 1/40 [00:03<02:12,  3.39s/it]\u001b[A\n",
      "  5%|▌         | 2/40 [00:06<02:07,  3.37s/it]\u001b[A\n",
      "  8%|▊         | 3/40 [00:10<02:04,  3.35s/it]\u001b[A\n",
      " 10%|█         | 4/40 [00:13<02:00,  3.36s/it]\u001b[A\n",
      " 12%|█▎        | 5/40 [00:16<01:56,  3.34s/it]\u001b[A\n",
      " 15%|█▌        | 6/40 [00:20<01:53,  3.35s/it]\u001b[A\n",
      " 18%|█▊        | 7/40 [00:23<01:51,  3.38s/it]\u001b[A\n",
      " 20%|██        | 8/40 [00:26<01:47,  3.37s/it]\u001b[A\n",
      " 22%|██▎       | 9/40 [00:30<01:43,  3.35s/it]\u001b[A\n",
      " 25%|██▌       | 10/40 [00:33<01:40,  3.36s/it]\u001b[A\n",
      " 28%|██▊       | 11/40 [00:36<01:36,  3.34s/it]\u001b[A\n",
      " 30%|███       | 12/40 [00:40<01:33,  3.34s/it]\u001b[A\n",
      " 32%|███▎      | 13/40 [00:43<01:30,  3.35s/it]\u001b[A\n",
      " 35%|███▌      | 14/40 [00:47<01:31,  3.51s/it]\u001b[A\n",
      " 38%|███▊      | 15/40 [00:51<01:30,  3.62s/it]\u001b[A\n",
      " 40%|████      | 16/40 [00:54<01:26,  3.58s/it]\u001b[A\n",
      " 42%|████▎     | 17/40 [00:58<01:21,  3.53s/it]\u001b[A\n",
      " 45%|████▌     | 18/40 [01:01<01:17,  3.53s/it]\u001b[A\n",
      " 48%|████▊     | 19/40 [01:05<01:15,  3.59s/it]\u001b[A\n",
      " 50%|█████     | 20/40 [01:09<01:12,  3.61s/it]\u001b[A\n",
      " 52%|█████▎    | 21/40 [01:12<01:09,  3.63s/it]\u001b[A\n",
      " 55%|█████▌    | 22/40 [01:17<01:08,  3.81s/it]\u001b[A\n",
      " 57%|█████▊    | 23/40 [01:20<01:04,  3.82s/it]\u001b[A\n",
      " 60%|██████    | 24/40 [01:24<01:00,  3.75s/it]\u001b[A\n",
      " 62%|██████▎   | 25/40 [01:27<00:54,  3.63s/it]\u001b[A\n",
      " 65%|██████▌   | 26/40 [01:31<00:50,  3.60s/it]\u001b[A\n",
      " 68%|██████▊   | 27/40 [01:34<00:46,  3.59s/it]\u001b[A\n",
      " 70%|███████   | 28/40 [01:38<00:42,  3.57s/it]\u001b[A\n",
      " 72%|███████▎  | 29/40 [01:42<00:40,  3.64s/it]\u001b[A\n",
      " 75%|███████▌  | 30/40 [01:45<00:35,  3.54s/it]\u001b[A\n",
      " 78%|███████▊  | 31/40 [01:48<00:31,  3.48s/it]\u001b[A\n",
      " 80%|████████  | 32/40 [01:52<00:27,  3.47s/it]\u001b[A\n",
      " 82%|████████▎ | 33/40 [01:55<00:24,  3.50s/it]\u001b[A\n",
      " 85%|████████▌ | 34/40 [01:59<00:21,  3.61s/it]\u001b[A\n",
      " 88%|████████▊ | 35/40 [02:04<00:19,  3.81s/it]\u001b[A\n",
      " 90%|█████████ | 36/40 [02:07<00:15,  3.82s/it]\u001b[A\n",
      " 92%|█████████▎| 37/40 [02:11<00:11,  3.75s/it]\u001b[A\n",
      " 95%|█████████▌| 38/40 [02:14<00:07,  3.63s/it]\u001b[A\n",
      " 98%|█████████▊| 39/40 [02:18<00:03,  3.55s/it]\u001b[A\n",
      " 75%|███████▌  | 3/4 [18:13<06:39, 399.46s/it]]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem = PAN-problem00003\n",
      "recall = 1.0\n",
      "accuracy = 0.675\n",
      "F1 = 0.8059701492537313\n",
      "\t Creating author-model for candidate00001 using 1500 features...\n",
      "\t\tfound 7 documents and 7858 relevant tokens.\n",
      "\t Creating author-model for candidate00002 using 1500 features...\n",
      "\t\tfound 7 documents and 7814 relevant tokens.\n",
      "\t Creating author-model for candidate00003 using 1500 features...\n",
      "\t\tfound 7 documents and 8925 relevant tokens.\n",
      "\t Creating author-model for candidate00004 using 1500 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tfound 7 documents and 7056 relevant tokens.\n",
      "\t Creating author-model for candidate00005 using 1500 features...\n",
      "\t\tfound 7 documents and 7376 relevant tokens.\n",
      "Evaluate on test dataset:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  6%|▋         | 1/16 [00:01<00:27,  1.82s/it]\u001b[A\n",
      " 12%|█▎        | 2/16 [00:03<00:25,  1.80s/it]\u001b[A\n",
      " 19%|█▉        | 3/16 [00:05<00:23,  1.78s/it]\u001b[A\n",
      " 25%|██▌       | 4/16 [00:07<00:21,  1.78s/it]\u001b[A\n",
      " 31%|███▏      | 5/16 [00:08<00:19,  1.78s/it]\u001b[A\n",
      " 38%|███▊      | 6/16 [00:10<00:17,  1.76s/it]\u001b[A\n",
      " 44%|████▍     | 7/16 [00:12<00:15,  1.75s/it]\u001b[A\n",
      " 50%|█████     | 8/16 [00:14<00:13,  1.74s/it]\u001b[A\n",
      " 56%|█████▋    | 9/16 [00:15<00:12,  1.75s/it]\u001b[A\n",
      " 62%|██████▎   | 10/16 [00:17<00:10,  1.75s/it]\u001b[A\n",
      " 69%|██████▉   | 11/16 [00:19<00:08,  1.77s/it]\u001b[A\n",
      " 75%|███████▌  | 12/16 [00:21<00:07,  1.76s/it]\u001b[A\n",
      " 81%|████████▏ | 13/16 [00:22<00:05,  1.76s/it]\u001b[A\n",
      " 88%|████████▊ | 14/16 [00:24<00:03,  1.75s/it]\u001b[A\n",
      " 94%|█████████▍| 15/16 [00:26<00:01,  1.77s/it]\u001b[A\n",
      "100%|██████████| 4/4 [18:42<00:00, 288.43s/it]]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem = PAN-problem00004\n",
      "recall = 1.0\n",
      "accuracy = 0.875\n",
      "F1 = 0.9333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lo_problem = pd.unique(data_train['dataset'])\n",
    "\n",
    "lo_F1 = []\n",
    "lo_acc = []\n",
    "\n",
    "for prob in tqdm(lo_problem[:4]) :\n",
    "    data_prob = data[data['dataset'] == prob]\n",
    "    \n",
    "    data_train = data_prob[data_prob['split'] == 'train']\n",
    "    #compute model for each problem:\n",
    "    model = AuthorshipAttributionMulti(data_prob, \n",
    "                                       vocab_size = 1500,  #uses 3000 most frequent ngram\n",
    "                                       stbl = True,  #type of HC statistic\n",
    "                                      ngram_range = (1,3), #mono-, bi-, and tri- grams\n",
    "                                       flat=True  # compress counts in each corpus (faster)\n",
    "                                                )\n",
    "    \n",
    "    #attribute test documents:\n",
    "    print(\"Evaluate on test dataset:\")\n",
    "    data_test = data_prob[data_prob['split'] == 'test']\n",
    "    lo_test_docs = pd.unique(data_test.doc_id)\n",
    "    df = pd.DataFrame() #save results in this dataframe\n",
    "\n",
    "    for doc in tqdm(lo_test_docs) :\n",
    "        sm = data_test[data_test.doc_id == doc]\n",
    "\n",
    "        pred,_ = model.predict(sm.text.values[0], unk_thresh = 1e6) \n",
    "                # can use 'unk_thresh' to get '<UNK>' instead of the name \n",
    "                # of the corpus with smallest HC in the case when the smallest\n",
    "                # HC is above 'unk_thresh'. \n",
    "\n",
    "        auth = sm.author.values[0]\n",
    "        df = df.append({'doc_id' : doc,\n",
    "                   'author' : auth,\n",
    "                   'predicted' : pred,\n",
    "                  }, ignore_index = True)\n",
    "\n",
    "\n",
    "    # evaluate accuracy and F1 score\n",
    "    df_r = df[df.predicted != '<UNK>']\n",
    "    recall = len(df_r) / len(df)\n",
    "    acc = np.mean((df_r.predicted == df_r.author).values)\n",
    "    \n",
    "    print(\"problem = {}\".format(prob))\n",
    "    print(\"recall = {}\".format(recall))\n",
    "    print(\"accuracy = {}\".format(acc))\n",
    "    print(\"F1 = {}\".format(2*recall*acc / (recall + acc)))\n",
    "    lo_F1 += [2*recall*acc / (recall + acc)]\n",
    "    lo_acc += [acc]\n",
    "\n",
    "#prob1: |W| = 3000, ng = (1,3) --> F1 = 0.6017, acc = 0.43 \n",
    "#       |W| = 1500, ng = (1,3) --> F1 = 0.672, acc = 0.5063\n",
    "#prob2: |W| = 3000, ng = (1,3) --> F1 = 0.666, acc = 0.5 \n",
    "#       |W| = 1500, ng = (1,3) --> F1 = 0.67857, acc = 0.5135\n",
    "#prob3: |W| = 3000, ng = (1,3) --> F1 = 0.8732, acc = 0.775\n",
    "#       |W| = 1500, ng = (1,3) --> F1 = 0.80597014, acc = 0.675\n",
    "#prob4: |W| = 3000, ng = (1,3) --> F1 = 0.857, acc = 0.75\n",
    "#       |W| = 1500, ng = (1,3) --> F1 = 0.933, acc = 0.857\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Multi-author with head-to-head comparisons</h2>\n",
    "\n",
    "Compare each pair of corpora. Use only distinguishing features of two corpora in testing. Attribute tested docuement to whichever corpus has most number of wins in all pairwise comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 author-pairs\n",
      "MultiBinaryAuthorModel: Creating model for candidate00001 vs candidate00002\n",
      "\t Creating author-model for candidate00001 using 1774 features\n",
      "\t\tfound 7 documents and 5715 relevant tokens\n",
      "\t Creating author-model for candidate00002 using 1774 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|█         | 1/10 [00:03<00:29,  3.28s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tfound 7 documents and 5549 relevant tokens\n",
      "MultiBinaryAuthorModel: Creating model for candidate00001 vs candidate00003\n",
      "\t Creating author-model for candidate00001 using 1811 features\n",
      "\t\tfound 7 documents and 5715 relevant tokens\n",
      "\t Creating author-model for candidate00003 using 1811 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|██        | 2/10 [00:06<00:25,  3.17s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tfound 7 documents and 5753 relevant tokens\n",
      "MultiBinaryAuthorModel: Creating model for candidate00001 vs candidate00004\n",
      "\t Creating author-model for candidate00001 using 2169 features\n",
      "\t\tfound 7 documents and 5715 relevant tokens\n",
      "\t Creating author-model for candidate00004 using 2169 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|███       | 3/10 [00:09<00:22,  3.23s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tfound 7 documents and 6041 relevant tokens\n",
      "MultiBinaryAuthorModel: Creating model for candidate00001 vs candidate00005\n",
      "\t Creating author-model for candidate00001 using 1682 features\n",
      "\t\tfound 7 documents and 5715 relevant tokens\n",
      "\t Creating author-model for candidate00005 using 1682 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|████      | 4/10 [00:12<00:18,  3.14s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tfound 7 documents and 5019 relevant tokens\n",
      "MultiBinaryAuthorModel: Creating model for candidate00002 vs candidate00003\n",
      "\t Creating author-model for candidate00002 using 1867 features\n",
      "\t\tfound 7 documents and 5549 relevant tokens\n",
      "\t Creating author-model for candidate00003 using 1867 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|█████     | 5/10 [00:15<00:15,  3.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tfound 7 documents and 5753 relevant tokens\n",
      "MultiBinaryAuthorModel: Creating model for candidate00002 vs candidate00004\n",
      "\t Creating author-model for candidate00002 using 2265 features\n",
      "\t\tfound 7 documents and 5549 relevant tokens\n",
      "\t Creating author-model for candidate00004 using 2265 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|██████    | 6/10 [00:18<00:12,  3.08s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tfound 7 documents and 6041 relevant tokens\n",
      "MultiBinaryAuthorModel: Creating model for candidate00002 vs candidate00005\n",
      "\t Creating author-model for candidate00002 using 1782 features\n",
      "\t\tfound 7 documents and 5549 relevant tokens\n",
      "\t Creating author-model for candidate00005 using 1782 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████   | 7/10 [00:21<00:09,  3.10s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tfound 7 documents and 5019 relevant tokens\n",
      "MultiBinaryAuthorModel: Creating model for candidate00003 vs candidate00004\n",
      "\t Creating author-model for candidate00003 using 2229 features\n",
      "\t\tfound 7 documents and 5753 relevant tokens\n",
      "\t Creating author-model for candidate00004 using 2229 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|████████  | 8/10 [00:25<00:06,  3.38s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tfound 7 documents and 6041 relevant tokens\n",
      "MultiBinaryAuthorModel: Creating model for candidate00003 vs candidate00005\n",
      "\t Creating author-model for candidate00003 using 1782 features\n",
      "\t\tfound 7 documents and 5753 relevant tokens\n",
      "\t Creating author-model for candidate00005 using 1782 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|█████████ | 9/10 [00:28<00:03,  3.27s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tfound 7 documents and 5019 relevant tokens\n",
      "MultiBinaryAuthorModel: Creating model for candidate00004 vs candidate00005\n",
      "\t Creating author-model for candidate00004 using 2170 features\n",
      "\t\tfound 7 documents and 6041 relevant tokens\n",
      "\t Creating author-model for candidate00005 using 2170 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 10/10 [00:33<00:00,  3.63s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tfound 7 documents and 5019 relevant tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|▋         | 1/16 [00:07<01:50,  7.40s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|█▎        | 2/16 [00:12<01:32,  6.59s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 19%|█▉        | 3/16 [00:16<01:17,  5.99s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|██▌       | 4/16 [00:21<01:06,  5.57s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|███▏      | 5/16 [00:26<00:59,  5.42s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|███▊      | 6/16 [00:31<00:53,  5.34s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|████▍     | 7/16 [00:36<00:46,  5.12s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|█████     | 8/16 [00:41<00:41,  5.13s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████▋    | 9/16 [00:46<00:35,  5.12s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|██████▎   | 10/16 [00:50<00:29,  4.96s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████▉   | 11/16 [00:55<00:24,  4.91s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|███████▌  | 12/16 [01:00<00:19,  4.86s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|████████▏ | 13/16 [01:05<00:14,  4.85s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|████████▊ | 14/16 [01:09<00:09,  4.78s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 94%|█████████▍| 15/16 [01:14<00:04,  4.85s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 16/16 [01:19<00:00,  4.89s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [01:53<00:00, 113.07s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem = PAN-problem00004\n",
      "recall = 1.0\n",
      "accuracy = 0.5625\n",
      "F1 = 0.72\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lo_problem = pd.unique(data_train['dataset'])\n",
    "\n",
    "lo_F1 = []\n",
    "lo_acc = []\n",
    "\n",
    "for prob in tqdm(lo_problem[3:4]) :\n",
    "    data_prob = data[data['dataset'] == prob]\n",
    "    data_train = data_prob[data_prob['split'] == 'train']\n",
    "    \n",
    "    #compute model for each problem:\n",
    "    model = AuthorshipAttributionMultiBinary(data_prob, \n",
    "                                       vocab_size = 100,  #uses 3000 most frequent ngram\n",
    "                                       stbl = True,  #type of HC statistic\n",
    "                                      ngram_range = (1,3), #mono-, bi-, and tri- grams\n",
    "                                             reduce_features=True\n",
    "                                                )\n",
    "    \n",
    "    #attribute test documents:\n",
    "    data_test = data_prob[data_prob['split'] == 'test']\n",
    "    lo_test_docs = pd.unique(data_test.doc_id)\n",
    "    df = pd.DataFrame() #save results in this dataframe\n",
    "\n",
    "    for doc in tqdm(lo_test_docs) :\n",
    "        sm = data_test[data_test.doc_id == doc]\n",
    "\n",
    "        pred = model.predict(sm.text.values[0], method = 'HC') \n",
    "                # can use 'unk_thresh' to get '<UNK>' instead of the name \n",
    "                # of the corpus with smallest HC in the case when the smallest\n",
    "                # HC is above 'unk_thresh'. \n",
    "\n",
    "        auth = sm.author.values[0]\n",
    "        df = df.append({'doc_id' : doc,\n",
    "                   'author' : auth,\n",
    "                   'predicted' : pred,\n",
    "                  }, ignore_index = True)\n",
    "\n",
    "\n",
    "    # evaluate accuracy and F1 score\n",
    "    df_r = df[df.predicted != '<UNK>']\n",
    "    recall = len(df_r) / len(df)\n",
    "    acc = np.mean((df_r.predicted == df_r.author).values)\n",
    "    \n",
    "    print(\"problem = {}\".format(prob))\n",
    "    print(\"recall = {}\".format(recall))\n",
    "    print(\"accuracy = {}\".format(acc))\n",
    "    print(\"F1 = {}\".format(2*recall*acc / (recall + acc)))\n",
    "    lo_F1 += [2*recall*acc / (recall + acc)]\n",
    "    lo_acc += [acc]\n",
    "\n",
    "#prob4: F1 = 0.72, acc = 0.5625, |W| = 100, ng = (1,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
