{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>HC in Cross-Domain Authorship Attribution Challenge</H1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use HC-based test to attribute authorship in the PAN2018 cross-domain authorship attribution challenge https://pan.webis.de/clef18/pan18-web/author-identification.html#cross-domain\n",
    "- Only the English part (problems 1-4) of this challenge is considered. \n",
    "- We use a lemmatized version of the data obtained using the Stanford CoreNLP (https://stanfordnlp.github.io/CoreNLP/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import codecs\n",
    "from tqdm import tqdm\n",
    "\n",
    "#import auxiliary functions for python\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from AuthAttLib import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>Load Data</H2>\n",
    "The data below was obtained by lemmatizing the original challenge test data using the Stanford CoreNLP lemmatizer https://stanfordnlp.github.io/CoreNLP/. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data PAN2018 (after lemmatization using the Stanford NLP) \n",
    "raw_data = pd.read_csv(\"/Users/kipnisal/Google Drive/Data/PAN2018/PAN2018_lemmatized.csv\")\n",
    "raw_data.loc[:,'lem_text'] = raw_data.loc[:,'lem_text'] # + ' ' + raw_data.loc[:,'POS']\n",
    "data = raw_data.filter(['dataset', 'author', 'doc_id', 'lem_text']).rename(columns = {'lem_text' : 'text'})\n",
    "data.loc[:,'type'] = 'train'\n",
    "data.loc[data.doc_id.str.find('test')>-1,'type'] = 'test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore proper names, numbers, and some pronouns: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of proper names, pronouns, and capitalized words.\n",
    "with open(\"../Data/list_of_names.txt\") as f:\n",
    "    proper_names = f.read().split(', ')\n",
    "\n",
    "other_words = ['she', 'he', 'him', 'we', 'it', 'they', 'you', 'me', 'myself',\n",
    "               'am', 'is', 'are', 'be', 'was','were','her', 'his', 'their',\n",
    "               'theirs', 'our', 'ours', 'your', 'yours', 'sister', 'brother',\n",
    "               'dad', 'mom', 'husband', 'wife', 'mother', 'woman', 'women',\n",
    "               'daughter', 'father', 'aunt', 'child','wife','child', 'girl',\n",
    "               'girls', 'son','captain', 'colonel', 'lady', 'mr', 'mrs', 'miss',\n",
    "               'sir', 'gentleman', 'publius','st','god','lord', 'chapter',\n",
    "               'queen', 'goblins', 'wynn', 'pokemon']\n",
    "\n",
    "numbers = ['one','two','three','four','five','six','seven','eight','nine','ten',\n",
    "           'hundred', 'thousand', 'million', 'billion']\n",
    "\n",
    "words_to_ignore = proper_names + other_words + numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>Multi-author model</H2>\n",
    "\n",
    "- For each problem in the challenge, train a model and evaluate it over a test set. <br>\n",
    "- The following implementation opt not to use the UNNKOWN option, hence the recall is always 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data[data['type'] == 'train']\n",
    "\n",
    "lo_problem = pd.unique(data_train['dataset'])\n",
    "\n",
    "lo_F1 = []\n",
    "lo_acc = []\n",
    "\n",
    "for prob in tqdm(lo_problem[0:4]) :\n",
    "    data_prob = data_train[data_train['dataset'] == prob]\n",
    "    \n",
    "    #compute model for each problem:\n",
    "    model = AuthorshipAttributionMulti(data_prob, \n",
    "                                       vocab_size = 3000,  #uses 3000 most frequent ngram\n",
    "                                       stbl = True,  #type of HC statistic\n",
    "                                      ngram_range = (1,3), #mono-, bi-, and tri- grams\n",
    "                                     words_to_ignore = words_to_ignore # exclude these words\n",
    "                                                )\n",
    "    \n",
    "    #attribute test documents:\n",
    "    data_prob_test = data[(data['dataset'] == prob) & (data['type'] == 'test')]\n",
    "    lo_test_docs = pd.unique(data_prob_test.doc_id)\n",
    "    df = pd.DataFrame() #save results in this dataframe\n",
    "\n",
    "    for doc in tqdm(lo_test_docs) :\n",
    "        sm = data_prob_test[data_prob_test.doc_id == doc]\n",
    "\n",
    "        pred,_ = model.predict(sm.text.values[0], unk_thresh = 1e6) \n",
    "                # can use 'unk_thresh' to get '<UNK>' instead of the name \n",
    "                # of the corpus with smallest HC in the case when the smallest\n",
    "                # HC is above 'unk_thresh'. \n",
    "\n",
    "        auth = sm.author.values[0]\n",
    "        df = df.append({'doc_id' : doc,\n",
    "                   'author' : auth,\n",
    "                   'predicted' : pred,\n",
    "                  }, ignore_index = True)\n",
    "\n",
    "\n",
    "    # evaluate accuracy and F1 score\n",
    "    df_r = df[df.predicted != '<UNK>']\n",
    "    recall = len(df_r) / len(df)\n",
    "    acc = np.mean((df_r.predicted == df_r.author).values)\n",
    "    \n",
    "    print(\"problem = {}\".format(prob))\n",
    "    print(\"recall = {}\".format(recall))\n",
    "    print(\"accuracy = {}\".format(acc))\n",
    "    print(\"F1 = {}\".format(2*recall*acc / (recall + acc)))\n",
    "    lo_F1 += [2*recall*acc / (recall + acc)]\n",
    "    lo_acc += [acc]\n",
    "\n",
    "#prob1: F1 = 0.6017, acc = 0.43 |W| = 3000, ng = (1,3)\n",
    "#prob2: F1 = 0.666, acc = 0.5 |W| = 3000, ng = (1,3)\n",
    "#prob3: F1 = 0.8732, acc = 0.775, |W| = 3000, ng = (1,3)\n",
    "#prob4: F1 = 0.857, acc = 0.75, |W| = 3000, ng = (1,3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Multi-author with head-to-head comparisons</h2>\n",
    "\n",
    "Compare each pair of corpora. Use only distinguishing features of two corpora in testing. Attribute tested docuement to whichever corpus has most number of wins in all pairwise comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 author-pairs\n",
      "MultiBinaryAuthorModel: Creating model for candidate00001 vs candidate00002\n",
      "\t Creating author-model for candidate00001 using 1774 features\n",
      "\t\tfound 7 documents and 5715 relevant tokens\n",
      "\t Creating author-model for candidate00002 using 1774 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|█         | 1/10 [00:03<00:29,  3.28s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tfound 7 documents and 5549 relevant tokens\n",
      "MultiBinaryAuthorModel: Creating model for candidate00001 vs candidate00003\n",
      "\t Creating author-model for candidate00001 using 1811 features\n",
      "\t\tfound 7 documents and 5715 relevant tokens\n",
      "\t Creating author-model for candidate00003 using 1811 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|██        | 2/10 [00:06<00:25,  3.17s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tfound 7 documents and 5753 relevant tokens\n",
      "MultiBinaryAuthorModel: Creating model for candidate00001 vs candidate00004\n",
      "\t Creating author-model for candidate00001 using 2169 features\n",
      "\t\tfound 7 documents and 5715 relevant tokens\n",
      "\t Creating author-model for candidate00004 using 2169 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|███       | 3/10 [00:09<00:22,  3.23s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tfound 7 documents and 6041 relevant tokens\n",
      "MultiBinaryAuthorModel: Creating model for candidate00001 vs candidate00005\n",
      "\t Creating author-model for candidate00001 using 1682 features\n",
      "\t\tfound 7 documents and 5715 relevant tokens\n",
      "\t Creating author-model for candidate00005 using 1682 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|████      | 4/10 [00:12<00:18,  3.14s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tfound 7 documents and 5019 relevant tokens\n",
      "MultiBinaryAuthorModel: Creating model for candidate00002 vs candidate00003\n",
      "\t Creating author-model for candidate00002 using 1867 features\n",
      "\t\tfound 7 documents and 5549 relevant tokens\n",
      "\t Creating author-model for candidate00003 using 1867 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|█████     | 5/10 [00:15<00:15,  3.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tfound 7 documents and 5753 relevant tokens\n",
      "MultiBinaryAuthorModel: Creating model for candidate00002 vs candidate00004\n",
      "\t Creating author-model for candidate00002 using 2265 features\n",
      "\t\tfound 7 documents and 5549 relevant tokens\n",
      "\t Creating author-model for candidate00004 using 2265 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|██████    | 6/10 [00:18<00:12,  3.08s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tfound 7 documents and 6041 relevant tokens\n",
      "MultiBinaryAuthorModel: Creating model for candidate00002 vs candidate00005\n",
      "\t Creating author-model for candidate00002 using 1782 features\n",
      "\t\tfound 7 documents and 5549 relevant tokens\n",
      "\t Creating author-model for candidate00005 using 1782 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████   | 7/10 [00:21<00:09,  3.10s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tfound 7 documents and 5019 relevant tokens\n",
      "MultiBinaryAuthorModel: Creating model for candidate00003 vs candidate00004\n",
      "\t Creating author-model for candidate00003 using 2229 features\n",
      "\t\tfound 7 documents and 5753 relevant tokens\n",
      "\t Creating author-model for candidate00004 using 2229 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|████████  | 8/10 [00:25<00:06,  3.38s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tfound 7 documents and 6041 relevant tokens\n",
      "MultiBinaryAuthorModel: Creating model for candidate00003 vs candidate00005\n",
      "\t Creating author-model for candidate00003 using 1782 features\n",
      "\t\tfound 7 documents and 5753 relevant tokens\n",
      "\t Creating author-model for candidate00005 using 1782 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|█████████ | 9/10 [00:28<00:03,  3.27s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tfound 7 documents and 5019 relevant tokens\n",
      "MultiBinaryAuthorModel: Creating model for candidate00004 vs candidate00005\n",
      "\t Creating author-model for candidate00004 using 2170 features\n",
      "\t\tfound 7 documents and 6041 relevant tokens\n",
      "\t Creating author-model for candidate00005 using 2170 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 10/10 [00:33<00:00,  3.63s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tfound 7 documents and 5019 relevant tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|▋         | 1/16 [00:07<01:50,  7.40s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|█▎        | 2/16 [00:12<01:32,  6.59s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 19%|█▉        | 3/16 [00:16<01:17,  5.99s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|██▌       | 4/16 [00:21<01:06,  5.57s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|███▏      | 5/16 [00:26<00:59,  5.42s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|███▊      | 6/16 [00:31<00:53,  5.34s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|████▍     | 7/16 [00:36<00:46,  5.12s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|█████     | 8/16 [00:41<00:41,  5.13s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████▋    | 9/16 [00:46<00:35,  5.12s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|██████▎   | 10/16 [00:50<00:29,  4.96s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████▉   | 11/16 [00:55<00:24,  4.91s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|███████▌  | 12/16 [01:00<00:19,  4.86s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|████████▏ | 13/16 [01:05<00:14,  4.85s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|████████▊ | 14/16 [01:09<00:09,  4.78s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 94%|█████████▍| 15/16 [01:14<00:04,  4.85s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 16/16 [01:19<00:00,  4.89s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [01:53<00:00, 113.07s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem = PAN-problem00004\n",
      "recall = 1.0\n",
      "accuracy = 0.5625\n",
      "F1 = 0.72\n"
     ]
    }
   ],
   "source": [
    "data_train = data[data['type'] == 'train']\n",
    "\n",
    "lo_problem = pd.unique(data_train['dataset'])\n",
    "\n",
    "lo_F1 = []\n",
    "lo_acc = []\n",
    "\n",
    "for prob in tqdm(lo_problem[3:4]) :\n",
    "    data_prob = data_train[data_train['dataset'] == prob]\n",
    "    \n",
    "    #compute model for each problem:\n",
    "    model = AuthorshipAttributionMultiBinary(data_prob, \n",
    "                                       vocab_size = 100,  #uses 3000 most frequent ngram\n",
    "                                       stbl = True,  #type of HC statistic\n",
    "                                      ngram_range = (1,3), #mono-, bi-, and tri- grams\n",
    "                                     words_to_ignore = words_to_ignore # exclude these words\n",
    "                                                )\n",
    "    \n",
    "    #attribute test documents:\n",
    "    data_prob_test = data[(data['dataset'] == prob) & (data['type'] == 'test')]\n",
    "    lo_test_docs = pd.unique(data_prob_test.doc_id)\n",
    "    df = pd.DataFrame() #save results in this dataframe\n",
    "\n",
    "    for doc in tqdm(lo_test_docs) :\n",
    "        sm = data_prob_test[data_prob_test.doc_id == doc]\n",
    "\n",
    "        pred = model.predict(sm.text.values[0], method = 'HC') \n",
    "                # can use 'unk_thresh' to get '<UNK>' instead of the name \n",
    "                # of the corpus with smallest HC in the case when the smallest\n",
    "                # HC is above 'unk_thresh'. \n",
    "\n",
    "        auth = sm.author.values[0]\n",
    "        df = df.append({'doc_id' : doc,\n",
    "                   'author' : auth,\n",
    "                   'predicted' : pred,\n",
    "                  }, ignore_index = True)\n",
    "\n",
    "\n",
    "    # evaluate accuracy and F1 score\n",
    "    df_r = df[df.predicted != '<UNK>']\n",
    "    recall = len(df_r) / len(df)\n",
    "    acc = np.mean((df_r.predicted == df_r.author).values)\n",
    "    \n",
    "    print(\"problem = {}\".format(prob))\n",
    "    print(\"recall = {}\".format(recall))\n",
    "    print(\"accuracy = {}\".format(acc))\n",
    "    print(\"F1 = {}\".format(2*recall*acc / (recall + acc)))\n",
    "    lo_F1 += [2*recall*acc / (recall + acc)]\n",
    "    lo_acc += [acc]\n",
    "\n",
    "#prob4: F1 = 0.72, acc = 0.5625, |W| = 100, ng = (1,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
